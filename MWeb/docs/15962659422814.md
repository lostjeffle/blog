title:'Feature - Plugging'
## Feature - Plugging


### Concept

#### introduction

queue plugging 特性最初来源于 HDD 设备，本身 request queue 的一个重要作用就是将过去一段时间上层下发的 request 缓存起来，而不是立即下发给 HDD 设备处理；这样当上层下发一个 bio 的时候，尽可能地将 bio 与之前缓存的 pending request 相合并，从而减小磁头抖动，以提升设备吞吐

request queue 是 per 设备的，所有进程下发的 IO 都会缓存到 request queue 中，但同时观察到以下现象，即一个进程在执行文件操作时有时会下发多个连续的 IO，由于这些 IO 的 sector 地址是连续的，因而在这些 IO 添加到到 request queue 之前，将这些 IO 缓存到一个 per-process 的链表中，这样同一个进程下发的多个连续 IO 就可以在添加到 request queue 之前尽可能地合并，从而提升设备的吞吐

这一特性就称为 queue plugging，当开启该特性时，上层提交的 IO request 会暂时缓存到 per-process 的 plug list 中，而不会马上添加到 request queue 中，之后下发的 IO 也会尝试与 plug list 中缓存的 IO 相合并，从而提高 IO 合并的几率


#### calling pattern

如下所示为 plug list 通常的调用模式

```c
struct blk_plug plug;

blk_start_plug(&plug);
/* submit bios */
blk_finish_plug(&plug);
```

- 在下发 IO 之前，调用 blk_start_plug() 为当前进程初始化 plug list
- 向 generic block layer 下发 IO
- 最后当批量 IO 下发完成后，

在连续多个 IO 请求提交结束后，文件系统可以显式地调用相关接口对 plug list 进行 flush 操作，即将 plug list 中的所有 pending request 添加到对应的 request queue 中


#### plug list

每个进程维护一个 per-process 的 plug list

```c
struct task_struct {
	struct blk_plug *plug;
	...
}
```

```c
struct blk_plug {
	struct list_head mq_list; /* requests */
	...
};
```

当上层通过 blk_start_plug() 开启 plug list 后，之后下发的 IO 会暂时缓存到 plug list 中，而不会马上添加到 request queue 中

需要注意的是，plug list 中缓存的是 request，当下发的 bio 不能与现存的 pending request 合并时，会将该 bio 封装为一个新的 request，当 plug list 特性开启时，这个新的 request 就会被添加到 plug list 中

同时由于 plug list 是 per-process 的，因而其中缓存的 request 实际上是来自各个 request queue 的


### working mode

queue plugging 特性来源于 HDD 时代，因而尽可能增加 IO 之间的合并，可以显著提升设备的吞吐，因为延迟添加到 request queue 而带来的延迟对于 HDD 设备本身就不低的延迟来说影响甚微

然而来到 SSD 甚至 NVMe 时代，由于设备乱序访问的性能与顺序访问的性能基本一致，设备对于 IO 合并的需求本就不强烈，此时 plug list 带来的延迟在设备本身的低延时面前开始凸显

因而对于 HDD 与 NVMe 设备，plug list 有两种工作模式

#### normal plugging

对于 single-queue 设备 (@q->nr_hw_queues 字段的值为 1)，或者 HDD 设备 (@q->queue_flags 标志位不包含 QUEUE_FLAG_NONROT，即当前设备是 rotational 设备)，plug list 会工作在 normal plugging 模式

该模式下 plug list 中可以包含多个来自同一个 request queue 的 request，最后当 plug list 中包含的 request 数量达到上限 (BLK_MAX_REQUEST_COUNT 即 16)，或者 plug list 中最近一个入队的 request 包含的数据量超过 BLK_PLUG_FLUSH_SIZE 即 128 KB 的时候，会自动触发 flush plug list 的操作

该模式的特点是 plug list 中可以包含尽可能多的 request (上限为 BLK_MAX_REQUEST_COUNT)，以提高 IO 合并的概率，因而适合 HDD 这类对 IO 合并有强需求的设备


#### limited plugging

对于 SSD 设备 (@q->queue_flags 标志位包含 QUEUE_FLAG_NONROT，同时通常为 multi-queue 设备即 @q->nr_hw_queues 字段的值大于 1)，plug list 会工作在 limited plugging 模式

该模式下 plug list 中每个 request queue 最多只能包含一个 request，如果当前下发的 bio 可以与这个唯一的 pending request 合并，那么就算发挥出了 plug list 的作用；否则当前下发的 bio 会封装为一个新的 request，这个新的 request 添加到 plug list 的同时，会将 plug list 中旧的 request 下发给 request queue，从而确保 plug list 中每个 request queue 最多只包含一个 request，这样就在 IO 吞吐和延迟之间获得平衡


### Routine

#### enqueue

当上层通过 blk_start_plug() 开启 plug list 后，之后下发的 IO 会暂时缓存到 plug list 中，而不会马上添加到 request queue 中

需要注意的是，plug list 中缓存的是 request，当下发的 bio 不能与现存的 pending request 合并时，会将该 bio 封装为一个新的 request，当 plug list 特性开启时，这个新的 request 就会被添加到 plug list 中

#### merge

开启 plug list 后，下发的 bio 会尝试与 plug list 中的 pending request 相合并

#### flush

plug list 的 flush 操作与其工作模式有关

之前介绍过在 normal plugging 模式下，虽然来自单个 request queue 的 request 的数量是不设上限的，但是整个 request queue 包含的 request 数量会存在上限；enqueue 过程中，在将当前下发的 request 添加到 plug list 之前，如果当前 plug list 中包含的 request 数量达到上限 (BLK_MAX_REQUEST_COUNT 即 16)，或者 plug list 中最近一个入队的 request 包含的数据量超过 BLK_PLUG_FLUSH_SIZE 即 128 KB 的时候，会自动触发 flush plug list 的操作

而在 limited plugging 模式下，由于 plug list 中每个 request queue 最多只能包含一个 request，因而整个 plug list 的长度相对可控，因而其 enqueue 过程中就没有类似 normal plugging 模式下的 flush 操作


此外上层在连续多个 IO 请求提交结束后，可以显式地调用 blk_finish_plug() 接口关闭 plug list，其中就会对 plug list 进行 flush 操作

此外当前运行进程被调度时，在 schedule() 中也会对当前进程的 plug list 进行 flush 操作，因而 plug mode 带来的延时即为进程的调度周期，这保证了 plug mode 带来的延时处在合理范围内
