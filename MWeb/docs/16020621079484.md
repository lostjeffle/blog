## MM - MISC

### process address space

#### lazy TLB flush

Linux 中使用 mm_struct 描述 process address space，因而每个用户进程都拥有一个唯一的 mm_struct 描述符，而内核线程 kernel thread 则不需要 mm_struct 描述符，因为内核线程没有 process address space，也不能访问 process address space

process context switch 过程中需要切换进程的 process address space，同时硬件需要执行 TLB flush 操作，即将 TLB 中上一个运行进程相关的 TLB 缓存清空，以准备执行下一个进程

例如由用户进程 A 切换到用户进程 B 时，需要在 MMU 中设置由用户进程 A 的 Translation Table 切换到用户进程 B 的 Translation Table，同时执行 TLB flush 操作以清空 TLB cache

TLB flush 操作需要一定的时间，当由用户进程 A 切换到内核线程 C 时，由于内核线程 C 不需要访问 process address space，只需要访问 kernel address space，而 kernel address space 总是全局可见的，因而在切换到内核线程时，实际不必执行 TLB flush 操作


因而 Linux 中使用 lazy TLB flush 机制，即在 process context switch 过程中，当切换到内核线程时，内核线程实际“借用”之前最近一个用户进程的 process address space，从而避免执行 TLB flush 操作，以提高性能

为了实现 lazy TLB flush

1. 在进程描述符中维护 mm 与 active_mm 两个成员，其中

- mm 成员描述 real process address space，用户进程的 mm 即为该进程使用的 process address space，内核线程的 mm 成员即为 NULL
- active_mm 成员描述 used process address space，process contetx switch 过程中实际根据切换进程描述符的 active_mm 成员判断是否需要执行 TLB flush 操作，用户进程的 active_mm 成员与 mm 成员的值一样，即为该进程使用的 process address space，而内核线程的 active_mm 成员即为进程切换时上一个运行进程的 active_mm 成员，即
    - 由用户进程 A 切换到内核线程 B 时，内核线程 B 的 active_mm 成员实际为用户进程 A 的 active_mm 成员的值即用户进程 A 的 mm 成员的值，此时内核线程 B 借用用户进程 A 的 process address space，以避免执行 TLB flush 操作
    - 之后再由内核线程 B 切换到内核线程 C 时，内核线程 C 的 active_mm 成员实际为内核线程 B 的 active_mm 成员的值即用户进程 A 的 mm 成员的值，此时内核线程 C 借用用户进程 A 的 process address space，以避免执行 TLB flush 操作


2. 在 mm_struct 中维护 mm_users 与 mm_count 两个计数

- mm_users 计数描述使用该 mm_struct 的 real user 的数量，即使用该 mm_struct 的所有 user thread 的数量，当 mm_users 计数变为 0 时，内核就会销毁该 process address space 中的所有映射关系，但是此时还不会销毁 mm_struct 结构，mm_struct 结构需要等到 mm_count 计数变为 0 再销毁
- mm_count 计数描述使用该 mm_struct 的 anonymous user 的数量，mm_count 的初始值为 1，因而 (mm_count - 1) 即为借用该 mm_struct 的所有 kernel thread 的数量，mm_count 计数变为 0 时，内核就会销毁 mm_struct 结构


#### mm_struct

> process address space descriptor

每个用户进程都有一个内存描述符 struct mm_struct 描述该进程的 process address space 的相关信息，其中主要维护该进程使用的所有 vma (virtual memory area)

那为什么内核线程就不需要内存描述符呢？这是因为内核对用户进程与内核线程的内核申请的策略是不同的

- 对于用户进程的内存申请，内核总是推迟实际的 page frame 的分配，等到用户进程访问分配的内存时，内核才会真正地为该进程分配对应的 page frame，这称为 page demanding，在实现这一策略时内核必须记录用户进程申请分配的内存对应的虚拟地址区间即 vma，而内存描述符则用于记录进程使用的所有 vma，即用户进程在申请内存分配时，内核实际为用户进程申请的内存分配一段对应的 vma 并保存到用户进程的内存描述符中，而并不分配相应的 page frame，之后当用户进程访问该内存时，硬件触发 page fault，在 page fault handler 中会通过进程的内存描述符维护的所有 vma，检查对于触发 page fault 的地址，进程是否具有相应的访问权限，若进程具有相应的访问权限，则内核会执行 page demanding 操作，为进程分配相应的 page frame；也就是说，为了实现 page demanding，必须使用某个数据结构维护用户进程使用的所有 vma，这一数据结构即为 struct mm_struct
- 而对于内核线程的内存申请，内核总是不假思索地立即分配相应的 page frame，因而对于内核线程就没有维护其使用的所有 vma 的需求，因而也就不需要维护 struct mm_struct 结构

然而在 lazy TLB flush 策略中，内核线程的 active_mm 成员也都必须指定一个 mm_struct，而 process 0 即 idle process 工作时，尚不存在其他的用户进程，同时内存子系统也还没有初始化完成，因而 process 0 的内存描述符 init_mm 是静态定义的，init_mm 的 pgd 成员指向 swapper_pg_dir，即 global page table


> kernel stack

内核区分使用 kernel mode stack 与 user mode stack

- 对于用户进程，进程处于用户态时使用 user mode stack，当进程进入内核态时即切换使用 kernel mode stack
- 对于内核线程，线程只有 kernel mode stack，而没有 user mode stack

Linux 中将 kernel mode stack 与 user mode stack 分开是为了实现用户态与内核态的隔离，即内核必须确保 kernel mode stack 处于自己的完全控制之下，而 user mode stack 的错误不会影响到 kernel mode stack


process 0 的 kernel stack 也是静态定义的，ARM 32 架构下该 kernel stack 的大小为 8192 字节，即 2 个 page frame

每个进程在创建时都会通过 alloc_thread_info_node() 创建自身进程使用的 kernel mode stack，ARM 32 架构下创建的 kernel mode stack 的大小为 2 * 4KB，kernel mode stack 的地址保存在 process descriptor 的 stack 成员

因而所有进程，包括用户进程与内核线程，其 kernel stack 的大小均是一样的，在 ARM 32 架构下其 kernel stack 的大小均为 2 * 4KB


> user stack

process 0 调用 kernel_thread() 创建 process 1 init 进程，此时 init kernel thread 的 kernel stack 为 2 page frame，没有 user stack

之后 init thread 调用 do_execve()使 process 1 运行用户态的 init 程序，并切换到用户态，其中会调用 bprm_mm_init() 为用户态的 process 1 创建一个新的 mm_struct，即内核态的 process 1 使用的 address space 为内核线程共用的 init_mm，而在之后用户态的 process 1 会切换到新创建的 mm_struct

之后 do_execve()中调用 ELF format handler 过程中会调用 setup_arg_pages()，其中会在用户态的 process 1 的 process address space 中分配用户栈 user mode stack 使用的 vma，用户栈为 32 * 4KB 大小，user stack 使用的 page frame 的分配依赖于 page demanding


每个用户态进程在创建时，在 load 过程中都会创建自身进程的 mm_struct，同时在自身进程的 process address space 中分配用户栈 user mode stack 使用的 vma，其大小为 32 * 4KB 


因而所有进程，包括用户进程与内核线程，其 user stack 的大小均是一样的，即 32 * 4KB


> mm_struct and page table

每个用户进程都具有各自独立的 process address space descriptor 即 mm_struct

而所有内核线程实际借用用户进程的 mm_struct 或者 init_mm

init_mm 的 pgd 成员指向 swapper_pg_dir，即所有内核线程都共用 global page table

而新创建的用户进程在执行 exec() syscall 以执行一个新的程序时，内核会为该进程创建该进程专用的 mm_struct，同时分配该进程专用的 page table，该 page table 在一开始是 global page table 的简单拷贝，由于 global page table 中 process address space 相关的 table entry 均为空，因而用户进程的 page table 在初始化完成后，其 process address space 相关的 table entry 为空，而 kernel address space 相关的 table entry 即为 global page table 的简单拷贝

因而 global page table 负责维护 kernel address space 相关的 table entry，所有其他用户进程使用的 page table 的 kernel address space 相关的 table entry 都会与 global page table 中的 kernel address space 相关的 table entry 保持同步

而每个用户进程各自维护的 page table 则用于维护各自进程的 process address space 相关的 table entry





### MISC

#### Kernel Thread Memory Descriptor

每个 regular process 都拥有一个内存描述符，进程描述符的 active_mm 字段指向其对应的内存描述符

而对于 kernel thread，其只能运行在 kernel mode 下，同时其只能访问 TASK_SIZE（同 PAGE_OFFSET，32 bit 架构下为 0xc0000000）以上的虚拟地址空间，因而 page table 中 TASK_SIZE 以下的虚拟地址空间对 kernel thread 完全无效，同时所有的 kernel thread 必须共用同一份 kernel page table；此外在 kernel mode 下也没有 vma 的概念，因而内存描述符中的大部分成员对于 kernel thread 无效

由于 page table 中 TASK_SIZE 以下的虚拟地址空间对 kernel thread 完全无效，因而为了减少 TLB/cache flush 从而提高性能，kernel thread 使用调度前上一个进程的内存描述符，即当调度执行该内核线程时，使用调度前进程的进程描述符的 active_mm 字段赋值给当前内核线程对应的进程描述符的 active_mm 字段

此时当内核线程调用 vmalloc 等操作对 TASK_SIZE 以上的虚拟地址相关的页表项进行修改时，由于所有进程各自维护的页表必须对 TASK_SIZE 以上的虚拟地址相关的页表项保持一致，因而必须通过某种机制更新所有进程的页表

实际上当用户进程进入 kernel mode 或 kernel thread 调用 vmalloc 等操作修改页表时，实际在 init_mm address space 的 swapper_pg_dir page global directory 即 master kernel page table 下修改相应的页表项，之后当用户进程需要访问修改的页表项对应的 page frame 时，由于当前进程自身的 process page table 中并没有该虚拟地址对应的页表项，因而会发生 page fault，此时 page fault handler 会在 master kernel page table 中查找是否存在该虚拟地址对应的页表项，当存在时 handler 会将 master kernel page table 中相关的页表项复制到当前进程的 process page table 中，之后进程可以正常访问对应的 page frame
