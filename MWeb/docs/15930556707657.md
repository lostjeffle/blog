## Writeback - 1 Core Routine


### Introduction

#### history

当进程对 page cache 中 page 的数据进行修改时，该 page 就被标记为 dirty page，VFS 需要将 dirty page 的数据写回到对应的 block device 中，从而将进程对数据的修改同步到 block device

Linux 执行 deffered dirty page writeback，即尽可能推迟 dirty page 的回写操作，这样可以将进程的多个修改操作整合为一个 hardware IO transfer，从而提高 block IO 的性能；但是 dirty page 的回写操作也不能推迟太久，以防止系统突然断电造成数据的丢失，或者 dirty page 占用过多内存而导致内存紧张


在 Linux 2.6.32 之前，VFS 使用 pdflush thread 负责管理 dirty page writeback 操作，但是该机制中 pdflush thread 负责所有 gendisk 的 dirty page writeback 操作，因而需要在多个 gendisk 之间来回切换

2.6.32 之后改为 per-BDI flush thread 机制，其中 VFS 使用 bdi-default thread 与 per-gendisk flush-x:y thread 完成 dirty page writeback 操作，该机制中前者相当于是管理线程，而后者相当于是工作线程。bdi-default thread 会为每个 gendisk 创建一个专门的 flush-x:y thread，其中 x 为该 gendisk 的 major number，y 为该 gendisk 的 minor number，这样每个 flush-x:y thread 都只负责一个 gendisk 的 dirty page writeback 操作，这样可以提高 IO 性能


per-BDI flush thread 机制中，在初始化阶段 bdi-default thread 被创建，bdi-default thread 每隔 5s 被唤醒，检查每个 bdi 中是否有过多的 dirty page，若 bdi 中存在过多的 dirty page 则该 bdi 特定的 flush thread 尚未被创建时，bdi-default thread 则会为该 bdi 创建一个 flush thread，之后 bdi-default thread 重新进入睡眠状态

per-BDI flush thread 每隔 5s 被唤醒，检查对应的 bdi 是否存在过多的 dirty page，若 bdi 中存在过多的 dirty page 则该 flush thread 执行相应的 writeback 操作，否则该 flush thread 重新进入睡眠状态；当 flush thread 连续 5 分钟不必执行 writeback 操作时，该 flush thread 则会结束运行，之后当该 bdi 产生新的 dirty page 需要处理时，bdi-default thread 会重新为该 bdi 创建一个 flush thread


per-BDI flush thread 机制中，即使当前没有任何 dirty page 需要处理，bdi-default thread 与 flush thread 都会定期被唤醒以轮询当前是否存在过多的 dirty page，频繁的唤醒操作不利于 power management，例如当设备处于 idle 状态进入 deep sleep mode 时，flush thread 的唤醒会使系统退出 sleep mode


因而在 3.10.0 之后，Linux 使用 workqueue 代替之前的 per-BDI flush thread 机制，此时由 workqueue 自身的 worker pool 负责 flush thread 的创建与销毁

- 在初始化过程中会创建 writeback workqueue，该 workqueue 创建过程中会创建一个 initial worker thread，此时该 worker thread 处于空闲即睡眠状态
- 之后在 write syscall 中若当前 dirty page 达到一定比例时就会将当前进程当前正在处理的 write operation 对应的 file 的 bdi 添加到 writeback workqueue 中，writeback workqueue 会自动唤醒相应的 worker thread 执行该 bdi 的 dirty page writeback 操作
- 此时由 writeback workqueue 负责控制 worker thread 的唤醒、创建与销毁

因而 per-BDI flush thread 机制中相当于使用轮询的方式，定期唤醒相应的线程以检查当前是否存在过多 dirty page，而 writeback workqueue 机制中若当前执行写操作的文件存在过多 dirty page，那么该文件被添加到 writeback workqueue 的时候，才会自动唤醒对应的 worker thread，从而避免了 per-BDI flush thread 机制中，即使当前没有任何 dirty page，相应的线程也会定期被唤醒的问题


#### writeback entry

在介绍 writeback 框架的各种概念之前，有必要先简单介绍一下回写操作的发起者，实际上有多个入口都会发起回写操作

- 用户调用 sync 主动发起回写操作，对系统中的某个 gendisk、或某个文件系统、或某个文件执行回写操作
- 当系统内存紧张而发生内存回收时，会发起回写操作，以回收一定数量的 page frame
- write 系统调用中，系统范围内 dirty page 数量超过一定阈值时，会发起回写操作
- periodic kupdate writeback，即同一个 dirty page 的回写操作不能延迟太久，因而需要定期对延迟达到一定时间的 dirty page 执行回写操作


### Concept

#### bdi

writeback 框架使用 struct backing_dev_info 结构即 bdi 来封装回写相关的所有字段，回写操作是以 gendisk 为单位的，因而每个 request queue（也就是每个 gendisk）有一个对应的 bdi 结构，保存在 @backing_dev_info 字段

```c
struct request_queue {
	struct backing_dev_info	*backing_dev_info;
	...
}
```

writeback 框架使用 @bdi_list 全局链表管理系统中的所有 bdi，gendisk 注册过程中会调用 bdi_register_owner() 注册该 gendisk 对应的 bdi，其中就会将 bdi 添加到 bdi_list 链表中


#### writeback

此外 writeback 框架使用 struct bdi_writeback（简称 wb）来抽象执行 writeback 操作的主体；一开始 writeback 操作的粒度是 gendisk 的，此时一个 wb 就对应一个 gendisk，后来支持 cgroup writeback 之后，writeback 操作的粒度是 per-cgroup 的，此时一个 wb 就对应该 gendisk 下的一个 cgroup

wb 最重要的作用就是维护所有需要执行回写操作的 dirty inode，@b_io 链表就描述了该 wb 下所有的 dirty inode

```c
struct bdi_writeback {	
	struct list_head b_io;	 /* parked for writeback */
	...
};
```


在一开始每个 bdi 只有一个 wb，也就是内嵌在 bdi->@wb 字段的 wb，这个唯一的 wb 维护了对应的 gendisk 下的所有 dirty inode

```c
struct backing_dev_info {
	struct bdi_writeback wb;  /* the root writeback info for this bdi */
	...
}
```

后来为了支持 cgroup writeback，wb 就变成了 per-cgroup 的概念，也就是每个 cgroup 都有一个对应的 wb，这样才实现了每个 cgroup 都可以独立执行回写操作，此时 bdi 的 @wb_list 链表就维护了该 bdi 下所有 cgroup 对应的 wb，不包括 @wb 字段中内嵌的 wb

```c
struct backing_dev_info {
	struct list_head wb_list; /* list of all wbs */
	...
}
```


#### writeback work & writeback control

正如之前介绍的，回写操作有多个入口，而无论是用户调用 sync 主动发起回写，还是 kupdate 线程周期性回写操作，其实都可以抽象出一个数据结构来描述当前需要执行的回写操作

writeback 框架中正是使用 writeback work 抽象对某个 wb 的一次回写操作，回写操作的不同入口都是根据当前回写的发起者、类型、回写的任务等设置好 writeback work 之后，传递给相应的函数执行相应的回写操作

此外为了提升效率，writeback 框架中会将一个 writeback work 描述的回写任务拆分为多个子任务（例如每个子任务回写 1024 个 page），此时就使用 writeback control 抽象每个子任务

writeback work 和 writeback control 具有大部分相同的字段

```c
struct wb_writeback_work;
struct writeback_control;
```

##### reason

```c
struct wb_writeback_work {
	enum wb_reason reason;	 /* why was writeback initiated? */
	...
};
```

只有 writeback work 具有 @reason 字段，描述了当前的回写任务的发起者

```c
enum wb_reason {
	WB_REASON_BACKGROUND,
	WB_REASON_VMSCAN,
	WB_REASON_SYNC,
	WB_REASON_PERIODIC,
	...
};
```


此外 writeback work 和 writeback control 同时也维护了一系列的 bitfield 描述当前的回写任务的发起者

```c
struct wb_writeback_work {
	unsigned int for_kupdate:1;
	unsigned int for_background:1;
	unsigned int for_sync:1; /* sync(2) WB_SYNC_ALL writeback */
	...
};
```

```c
struct writeback_control {
	unsigned for_kupdate:1;	       /* A kupdate writeback */
	unsigned for_background:1;    /* A background writeback */
	unsigned for_reclaim:1;	       /* Invoked from the page allocator */
	unsigned for_sync:1;		       /* sync(2) WB_SYNC_ALL writeback */
	...
};
```

- 若当前是因为 sync 调用发起回写任务，那么 {.reason=WB_REASON_SYNC, .for_sync =1}
- 若当前是因为内存回收发起回写任务，那么 {.reason=WB_REASON_VMSCAN}
- 若当前是因为 dirty page 数量超过阈值而发起回写任务，那么 {.reason=WB_REASON_BACKGROUND, .for_background =1}
- 若当前是 kupdate 周期性回写，那么 {.reason=WB_REASON_PERIODIC, .for_kupdate =1}


##### sync_mode

writeback work 和 writeback control 都维护有 @sync_mode 字段描述回写过程中是否等待回写下发的 IO 完成

```c
struct wb_writeback_work {
	enum writeback_sync_modes sync_mode;
	...
}
```

```c
struct writeback_control {
	enum writeback_sync_modes sync_mode;
	...
};
```

回写过程中会对 dirty inode 依次执行数据与元数据的回写操作，@sync_mode 描述是否等待这些数据与元数据回写完成

```c
enum writeback_sync_modes {
	WB_SYNC_NONE,	/* Don't wait on anything */
	WB_SYNC_ALL,	/* Wait on every mapping */
};
```

如果 @sync_mode 为 WB_SYNC_NONE，那么回写过程中只是下发 IO，而不等待这些 IO 完成。由于回写过程中会先回写数据，再回写元数据，那么当 @sync_mode 为 WB_SYNC_NONE 时，回写数据过程中在下发 IO 之后，不等待这些回写数据的 IO 完成，就开始回写元数据，同时不等待这些回写元数据的 IO 完成，回写任务就宣告完成；也就是说在该模式下，在回写任务完成（即 __writeback_single_inode() 函数调用返回）时，只是说明相关的 IO 已经下发，但是并不能确保这些 IO 已经完成

而如果 @sync_mode 为 WB_SYNC_ALL，那么回写过程中，在回写数据的 IO 下发下去之后，会等待这些回写数据的 IO 完成，之后才会发起元数据的回写，同样地在回写元数据的 IO 完成之后，回写任务才会宣告完成


@sync_mode 只是描述回写过程中是否等待数据/元数据回写完成，而与当前的回写操作是否由 sync 系统调用发起并没有强相关

实际上 @reason 为 WB_REASON_SYNC 时，@sync_mode 也可以为 WB_SYNC_NONE，这里读者可能会疑惑为什么 sync 发起的回写任务，@sync_mode 可以为 WB_SYNC_NONE，也就是不等待回写的 IO 完成？

假设 sync 系统调用中，需要对每个 dirty inode 执行 WB_SYNC_ALL 模式的回写任务，那么每回写一个 dirty inode，都必须等待这个 inode 回写完成，之后才能继续回写下一个 dirty inode，这相当于是串行地回写每个 dirty inode，这种设计虽然可以实现 sync 语义，但是其效率可想而知

因而为了提升效率，sync 系统调用的实现实际上包含了两个回写任务

- 第一次执行 {.reason = WB_REASON_SYNC, .sync_mode = WB_SYNC_NONE} 的回写任务，不求等待回写的 IO 完成，只需要将回写的 IO 下发下去就好
- 第二次执行 {.reason = WB_REASON_SYNC, .sync_mode = WB_SYNC_ALL} 的回写任务，通过第二次下发的回写任务来确保所有回写的 IO 完成，从而确保 sync 调用的语义


而对于 {.reason = WB_REASON_SYNC, .sync_mode = WB_SYNC_ALL} 这种语义，也就是 1) sync 系统调用发起的回写任务，同时 2) 等待所有回写的 IO 完成这种情况，writeback work 和 writeback control 中还特地维护了一个 @for_sync 字段来描述这种情况


##### sb

前面描述过，dirty inode 链表维护在 wb 中，而 bdi 的 @wb_list 链表维护了对应的 gendisk 下的所有 wb；一个 gendisk 下可能存在多个分区，即一个 gendisk 下可能存在多个文件系统，此时 gendisk 下所有文件系统的所有 dirty inode 实际上是维护在同一个链表中的

有时候执行回写任务例如 syncfs 时，我们可能希望只对某个文件系统下的 dirty inode 执行回写操作，此时 writeback work 的 @sb 就描述对哪个文件系统下的 dirty inode 执行回写操作，如果该字段为 NULL，那么就会对该 gendisk 下的所有 dirty inode 执行回写操作

```c
struct wb_writeback_work {
	struct super_block *sb;
	...
};
```

只有 writeback work 维护有该字段


##### nr_pages/nr_to_write

writeback work 的 @nr_pages 字段，以及 writeback control 的 @nr_to_write 字段具有相同含义，都描述了当前任务需要回写的 dirty page 的数量

```c
struct wb_writeback_work {
	long nr_pages;
	...
};
```

```c
struct writeback_control {
	long nr_to_write;		/* Write this many pages, and decrement
					   this for each page written */
	...
};
```


##### range_start/range_end

```c
struct writeback_control {
	/*
	 * For a_ops->writepages(): if start or end are non-zero then this is
	 * a hint that the filesystem need only write out the pages inside that
	 * byterange.  The byte at `end' is included in the writeout request.
	 */
	loff_t range_start;
	loff_t range_end;
	...
};
```

writeback_control 的 @range_start/@range_end 参数描述了需要回写的 dirty buffer range 的范围，也就是 a_ops->writepages() 只需要对 inode 中 [range_start, range_end] 文件偏移范围内的 dirty buffer page 执行回写操作

只有 writeback control 维护有这两个字段


##### range_cyclic

如之前所述，@range_start/@range_end 参数描述了需要回写的 dirty range 在文件中的偏移范围，a_ops->writepages() 每次都是从 @range_start 偏移处开始寻找并回写 dirty page，如果一个文件中 dirty page 的数量大于 @nr_to_write 参数，那么每次对文件执行回写操作时，@range_start 偏移处起始的 dirty page 被回写的概率就会远远大于其他偏移位置处，这显然是不公平的

后来在 address_space 中维护了一个 @writeback_index 字段，该字段描述了上一次该 inode 回写过程中，最近一个执行回写操作的 dirty page 的文件偏移

```c
struct address_space {
	pgoff_t			writeback_index;/* writeback starts here */
	...
}
```


这样当 writeback_control 的 @range_cyclic 标志位为 1 时，搜寻 dirty page 的偏移范围就变成了 [@writeback_index, MAX]，这样就确保了在整个文件偏移范围内均匀地回收 dirty page

```c
struct writeback_control {
	unsigned range_cyclic:1;      /* range_start is cyclic */
	...
};
```


@writeback_index 的初始值为 0，只有在开启 @range_cyclic 标志位的时候，回写过程中才会更新 @writeback_index 字段

但是还有一个特例，就是 @range_start/@range_end 参数为 [0, LLONG_MAX] 也就是回写 inode 的所有 dirty page，但同时 a_ops->writepages() 中实际成功回写的 dirty page 数量小于 writeback_control 指定的 @nr_to_write 参数，此时即使没有开启 @range_cyclic，也会更新 @writeback_index 字段；这样调用者就可以再次发起一轮回写并开启 range_cyclic 标志，此时就可以直接从上一次的偏移位置处开始，继续回写剩余的 dirty page


### Writeback Core Routine

#### wb_writeback

执行回写操作的核心函数为 wb_writeback() 函数，描述对 @wb 执行回写操作，@work 参数描述了当前回写操作的类型，同时其中的一些参数会控制接下来回写操作的行为

```c
static long wb_writeback(struct bdi_writeback *wb,
			 struct wb_writeback_work *work)
```


简单介绍一下 wb_writeback() 的执行流程

实际上就是依次对该 wb 的 dirty inode 链表 (@wb->b_io) 中的所有 dirty inode 依次执行回写操作，目标是回写的 dirty page 数量达到 work->nr_pages

为了提升效率，wb_writeback() 中实际循环调用 writeback_sb_inodes()，每一轮 writeback_sb_inodes() 的连续运行时间不能超过 100 ms

```c
long wb_writeback(struct bdi_writeback *wb,
			 struct wb_writeback_work *work)
{
	for (;;) {
		if (work->nr_pages <= 0)
			break;
			
	   writeback_sb_inodes(work->sb, wb, work);
	   ...
	}
	...
}
```

```c
long writeback_sb_inodes(struct super_block *sb,
				struct bdi_writeback *wb,
				struct wb_writeback_work *work)
{
	while (!list_empty(&wb->b_io)) {
		...
		if (time_is_before_jiffies(start_time + HZ / 10UL))
			break;
		if (work->nr_pages <= 0)
			break;
		...
	}
	...
}
```

#### writeback_sb_inodes

wb_writeback() 中会调用 writeback_sb_inodes()，writeback_sb_inodes() 实际上是一个过渡函数，描述了对 @sb 文件系统下的 dirty inode 执行回写操作

```c
long writeback_sb_inodes(struct super_block *sb,
				struct bdi_writeback *wb,
				struct wb_writeback_work *work)
```

之前介绍过一个 gendisk 下可能存在多个分区，也就存在多个文件系统，wb_writeback() 中

- 如果传入的 @work->sb 不为空，说明当前只需要对这一个文件系统下的 dirty inode 执行回写操作，此时只是调用一次 writeback_sb_inodes(@work->sb, wb, work)
- 否则说明当前需要对 gendisk 下的所有 dirty inode 执行回写操作，此时会对该 gendisk 下的所有 superblock 依次调用 writeback_sb_inodes()


#### __writeback_single_inode

writeback_sb_inodes() 中会对 @sb 文件系统下的每个 dirty inode 调用 __writeback_single_inode()，即对单个 dirty inode 执行回写操作，以回收其中的 dirty page

同时 writeback_sb_inodes() 中还会将传入的 @work 描述的回写任务拆分为子任务即 writeback control，这些 writeback control 会传递给 __writeback_single_inode() 作为 @wbc 参数，以控制接下来回写操作的相关行为

```c
int __writeback_single_inode(struct inode *inode, struct writeback_control *wbc)
```

1. data writeback

首先需要对文件的数据，即 page cache 执行回写操作，其中调用 a_ops->writepages() 回调函数，在 address space 的 radix tree 中查找所有标记为 dirty page 的 buffer page，向 generic block layer 提交 WRITE IO transfer 以提交对这些 dirty page 的回写操作

wbc->nr_to_write 参数描述了需要回写的 dirty page 的数量，a_ops->writepages() 回调函数中就会致力于回写 @nr_to_write 数量的 dirty page

a_ops->writepages() 回调函数返回的时候，wbc->nr_to_write 参数描述了剩余仍然需要回写的 dirty page 的数量，此时 wbc->nr_to_write 参数的值有可能是大于零的，也就是没有完成 @nr_to_write 指定数量的 dirty page 的回写

由于 __writeback_single_inode() 中只会调用一次 a_ops->writepages() 回调函数，因而 __writeback_single_inode() 返回的时候，wbc->nr_to_write 参数的值也有可能是大于零的


2. metadata writeback

在文件的数据也就是 page cache 中的 dirty page 回写完成后（实际上就是 a_ops->writepages() 回调函数返回后），就会开始对文件的元数据执行回写操作，其中如果 inode 处于 I_DIRTY_SYNC/I_DIRTY_DATASYNC 状态，那么就会调用 a_ops->write_inode() 回调函数，对这个 inode 本身执行回写操作


这里需要注意 I_DIRTY_TIME 状态的 inode 的回写，一般情况下如果 inode 只是处于 I_DIRTY_TIME 状态，而没有 I_DIRTY_SYNC/I_DIRTY_DATASYNC 状态，那么一般是不会对这个 inode 执行回写操作的

只有当满足以下条件时才会对 I_DIRTY_TIME 状态的 inode 执行回写操作

- 这个 inode 处于 I_DIRTY_TIME 状态的同时，还处于 I_DIRTY_SYNC/I_DIRTY_DATASYNC 状态，也就是说 inode 中有除 atime/mtime/ctime 以外的其他字段需要更新，此时就需要回写该 inode
- 或者当前的 wbc->sync_mode 参数为 WB_SYNC_ALL
- 或者这个 inode 标记有 I_DIRTY_TIME_EXPIRED 状态，当初在将 @b_dirty/@b_dirty_time 链表下的 dirty inode 转移到 @b_io 链表的过程中，只有那些数据或元数据被更新的时刻 (@dirtied_when) 距离当前时刻已经超过 dirtytime_expire_interval 秒，也就是已经超时的 inode 才会转移到 @b_io 链表，此时这些 dirty inode 都会设置上 I_DIRTY_TIME_EXPIRED 状态
- 或者这个 inode 当初更新 atime/mtime/ctime 的时刻 (@dirtied_time_when) 距离当前时刻已经超过 dirtytime_expire_interval 秒，也就是需要回写已经超时的 inode


这里为了区分条件 3 和条件 4，需要正确理解 inode 的以下两个字段

```c
struct inode {
	unsigned long		dirtied_when /* jiffies of first dirtying */
	unsigned long		dirtied_time_when;
	...
}
```

@dirtied_time_when 描述 inode 的 atime/mtime/ctime 更新的时刻
@dirtied_when 描述 inode 更新的时刻，包括所有 metadata (I_DIRTY_SYNC/I_DIRTY_DATASYNC/I_DIRTY_TIME) 与 data (I_DIRTY_PAGES)

这两个字段在 __mark_inode_dirty() 中更新










