title:'Block - Queue Mapping'
## Block - Queue Mapping


### Queue Type

#### software queue

multiqueue 机制下，generic block layer 下发的 IO request 会首先缓存到 software queue，@cpu 描述该 software queue 所在的 CPU

```c
struct blk_mq_ctx {
	struct request_queue	*queue;
	unsigned int		cpu;
	...
};
```


每个 request queue 分配有一组 per-CPU software queue

```c
struct request_queue {
	/* sw queues */
	struct blk_mq_ctx __percpu *queue_ctx;
	unsigned int		nr_queues;
	...
}
```

@queue_ctx 指向 per-CPU software queue
@nr_queues 描述 software queue 的数量，其值实际上就是系统中 CPU 的数量


#### hardware dispatch queue

multiqueue 机制下，hardware dispatch queue 负责收集 per-CPU software queue 中缓存的所有 pending IO request，并将其下发给硬件设备的 hardware queue

```c
struct blk_mq_hw_ctx {
	unsigned int		numa_node;
	unsigned int		queue_num;
	...
};
```

@queue_num 描述该 hardware dispath queue 在所有 hardware dispath queue 中的编号
@numa_node 描述该 hardware dispath queue 所在的 NUMA node


每个 request queue 分配有一组 hardware dispatch queue

```c
struct request_queue {
	/* hw dispatch queues */
	struct blk_mq_hw_ctx	**queue_hw_ctx;
	unsigned int		nr_hw_queues;
	...
}
```

@queue_hw_ctx[] 数组描述 request queue 的一组 hardware dispath queue
@nr_hw_queues 描述 hardware dispath queue 的数量


### Queue-Mapping Mechanism

#### Single Queue-Mapping

multiqueue 机制下，每个 CPU 有一个 software queue，在某个 CPU 上运行的所有进程下发的 IO 都会缓存到该 software queue 中

同时每个 hardware queue 有一个对应的 hardware dispatch queue，hardware dispatch queue 中缓存的 IO 都会交给对应的硬件 hardware queue 处理

这一过程中每个 software queue 都需要对应一个 hardware dispatch queue，software queue 中缓存的所有 IO 都会交给对应的 hardware dispatch queue 处理


multiqueue 框架中就使用 queue mapping 来描述 software queue 与 hardware dispatch queue 之间的映射关系，我们将描述这种映射关系的数据结构称为 multiqueue map

一开始 multiqueue map 就实现为 tagset 中的一个 int 数组，即 @mq_map[]

```c
struct blk_mq_tag_set {
    unsigned int        *mq_map;
    ...
};
```

- index 为 CPU number
- @mq_map[cpu] 即为该 CPU 的 software queue 映射的 hardware dispatch queue 的编号


> relations

这个 multiqueue map 同时还保存在 request queue 的 @mq_map 字段，其实际上就指向 tagset->mq_map 指向的那个数组

```c
struct request_queue {
	unsigned int		*mq_map;
	...
}
```


此时 software queue 的 @index_hw 字段描述了该 software queue 映射的 hardware dispatch queue 的编号

```c
struct blk_mq_ctx {
	unsigned int		index_hw;
	...
};
```


一个 hardware dispatch queue 可以映射到多个 software queue

@ctxs[] 数组就描述了该 hardware dispatch queue 映射的所有 software queue

@nr_ctx 描述了该数组的长度，即该 hardware dispatch queue 映射的所有 software queue 的数量

```c
struct blk_mq_hw_ctx {
	struct blk_mq_ctx	**ctxs;
	unsigned int		nr_ctx;
	
	struct sbitmap		ctx_map;
	...
};
```

@ctx_map bitmap 描述了该 hardware dispatch queue 映射的所有 software queue 中，哪些 software queue 中存在有 pending request，request 添加到 software queue 的时候，会将 @ctx_map bitmap 中对应的 bit 置为 1


#### Multi Queue-Mapping

内核在 v5.0 版本引入 multi queue mapping 特性，该特性的本意是将不同类型的 IO 在 hardware queue 这一层相隔离，例如一部分 hardware queue 专门用于 read IO，另一部分 hardware queue 专门用于 write IO，这样在 hardware queue 层面实现读写分离

为了实现该特性，使用 struct blk_mq_queue_map 抽象一套 queue map，设备驱动负责为不同的 IO 类型分别实现对应的一套 queue map

```c
struct blk_mq_tag_set {
	struct blk_mq_queue_map map[HCTX_MAX_TYPES];
	unsigned int		nr_maps;
	...
}
```

@nr_maps 描述设备驱动实现了几套 queue map


struct blk_mq_queue_map 抽象一套 queue map

- 这一套 queue map 映射的 hardware queue 的范围为 [@queue_offset, @queue_offset + @nr_queues)
- @mq_map[cpu] 还是描述当前 CPU 的 software queue 映射的 hardware dispatch queue 的编号

```c
struct blk_mq_queue_map {
	unsigned int *mq_map;
	unsigned int nr_queues;
	unsigned int queue_offset;
};
```


目前 block layer 最多可以实现 HCTX_MAX_TYPES 即 3 套 queue map

```c
enum hctx_type {
	HCTX_TYPE_DEFAULT,
	HCTX_TYPE_READ,
	HCTX_TYPE_POLL,

	HCTX_MAX_TYPES,
};
```

- HCTX_TYPE_POLL 用于 poll 类型的 IO
- HCTX_TYPE_READ 用于 read 类型的 IO
- HCTX_TYPE_DEFAULT 则用于其他类型的 IO


此时 software queue 也维护一个 @hctxs[] 数组描述该 software queue 在各种 type 下对应的 hardware dispatch queue

```c
struct blk_mq_ctx {
	struct blk_mq_hw_ctx 	*hctxs[HCTX_MAX_TYPES];
	...
}
```


此时 block layer 通过 blk_mq_map_queue() 根据当前所在的 CPU、以及当前下发的 IO 类型，来决定使用哪个 hardware dispatch queue

```sh
submit_bio
    submit_bio_noacct
        __submit_bio_noacct_mq
            blk_mq_submit_bio
                __blk_mq_alloc_request
                    blk_mq_map_queue
```

blk_mq_map_queue() 的实现为

```sh
blk_mq_map_queue
	type = HCTX_TYPE_DEFAULT

	if (flags & REQ_HIPRI)
		type = HCTX_TYPE_POLL;
	else if ((flags & REQ_OP_MASK) == REQ_OP_READ)
		type = HCTX_TYPE_READ;
	
	return ctx->hctxs[type];
}
```


### Queue-Mapping Algorithm

multiqueue 框架下，hardware dispatch queue 与 hardware queue 通常是一一映射的关系

实际上 block layer 并不关心两者的映射关系，它们的映射关系实际上是由设备驱动自己定义的，设备驱动需要自己实现 hardware dispatch queue 的 index 到对应的 hardware queue 之间的映射关系

但实际上两者通常是一一映射的关系，例如对于 nvme 设备，其申请创建的 hardware dispatch queue 的数量即为该硬件设备的 hardware queue 的数量，同时 hardware dispatch queue 与 hardware queue 具有一一映射的关系


此外 software queue 与 hardware dispatch queue 之间通常是“多对一”或“一对一”的关系。当设备的 hardware queue 的数量超过系统中 CPU 的数量时，software queue 与 hardware dispatch queue 之间就是“一对一”的关系；而当设备的 hardware queue 的数量小于系统中 CPU 的数量时，software queue 与 hardware dispatch queue 之间就是“多对一”的关系，即此时多个 CPU 的 software queue 可以共用一个 hardware dispatch queue

blkdev driver 可以通过 mq_ops->map_queues() 回调函数自己指定 software queue 与 hardware dispatch queue 之间的映射，如果该回调函数未定义，那么就回退到 block layer 中默认的映射算法

```c
blk_mq_update_queue_map
	if (set->ops->map_queues) set->ops->map_queues
	else blk_mq_map_queues
```


#### default mapping algorithm

之前介绍过，当 mq_ops->map_queues() 回调函数未定义时，会回退到 block layer 中实现的默认的映射算法，该算法可以简单描述为

```
mq_map[cpu] = cpu % num_hardware_dispatch_queue
```

具体的逻辑是

```c
blk_mq_map_queues
    for_each_possible_cpu(cpu) {
    	/*
    	 * First do sequential mapping between CPUs and queues.
    	 * In case we still have CPUs to map, and we have some number of
    	 * threads per cores then map sibling threads to the same queue for
    	 * performace optimizations.
    	 */
    	if (cpu < nr_queues) {
    		map[cpu] = cpu % nr_queues
    	} else {
    		first_sibling = get_first_sibling(cpu);
    		if (first_sibling == cpu)
    			map[cpu] = cpu % nr_queues
    		else
    			map[cpu] = map[first_sibling];
    	}
    }
```

- 小于 nr_queues 的 cpunum 都是一一映射到硬件队列的，这样做是为了优先将硬件队列占满，防止出现没有CPU绑定的硬件队列，造成资源浪费
- 大于 nr_queues 的 cpunum，就会按照 mod 计算分配到各个硬件队列，同时一个原则是，processor 上的两个 thread 会优先映射为同一个硬件队列


v5.4 之后该算法有一些微调

```c
blk_mq_map_queues
	unsigned int q = 0

	for_each_possible_cpu(cpu) {
		/*
		 * First do sequential mapping between CPUs and queues.
		 * In case we still have CPUs to map, and we have some number of
		 * threads per cores then map sibling threads to the same queue
		 * for performance optimizations.
		 */
		if (q < nr_queues) {
			map[cpu] = q++ % nr_queues;
		} else {
			first_sibling = get_first_sibling(cpu);
			if (first_sibling == cpu)
				map[cpu] = q++ % nr_queues;
			else
				map[cpu] = map[first_sibling];
		}
	}
```

可以看到大体原则还是和之前一样的，只是在 mod 计算这部分有一些优化，之前在 mod 计算的时候，是用 "cpu % nr_queues"，这个时候考虑 nr_queues=6，CPU 0/1 互为 sibling，CPU 2/3 互为 sibling ... 的情况，此时

- CPU 0~5 先依次映射到 queue 0～5
- 接着 CPU 6 根据 mod 计算映射到 queue 0，其 sibling 即 CPU 7 也映射到 queue 0
- 接着同样的一对 sibling 即 CPU 8/9 映射到 queue 2

这就导致了硬件队列在所有 CPU 之间不均匀映射，但是 v5.4 之后的算法就可以解决这一问题


#### MSI-X mapping algorithm

之前介绍过 blkdev driver 可以通过 mq_ops->map_queues() 回调函数自己指定 software queue 与 hardware dispatch queue 之间的映射，通常基于 PCI 总线的块设备会走这一路径

这些基于 PCI 总线的块设备一般使用 MSI-X 作为中断机制，nvme/virtio-blk 设备在 probe 过程中都会调用 pci_alloc_irq_vectors_affinity() 来完成 MSI-X 中断的分配、初始化操作，一般优先为每个硬件队列分配单独的一个 MSI-X 中断，其中会决定分配的 MSI-X 中断的 CPU affinity，其算法也是类似于之前介绍的按照 CPU sibling 实现映射的算法

之后在设备驱动的 mq_ops->map_queues() 回调函数中，通常是根据硬件队列绑定的 MSI-X 中断的 CPU affinity，来决定该硬件队列映射到哪些 software queue，例如假设一个硬件队列绑定的 MSI-X 中断的 CPU affinity 是绑定到 CPU A/B/C 的，那么这个硬件队列就会映射到 CPU A/B/C 的 software queue；而如果设备不支持 MSI-X 中断，或者设备支持的 MSI-X 中断数量有限导致多个硬件队列需要共用一个 MSI-X 中断，那么此时就会回退到之前介绍的 default CPU mapping 算法来实现 hardware queue 与 software queue 之间的映射





