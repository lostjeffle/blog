title:'Waitqueue'
## Waitqueue


等待队列本质上是一个双向链表，由等待队列与等待队列节点构成

等待队列通常用于进程的睡眠等待，当进程进入睡眠状态时，为该进程分配一个等待队列节点，将睡眠进程保存到该节点中，之后将该节点添加到特定等待队列中；之后当睡眠进程等待的事件发生时，就可以通过等待队列找到并唤醒睡眠进程


### Concept

#### wait queue

wait_queue_head 抽象等待队列

```c
struct wait_queue_head {
	spinlock_t		lock;
	struct list_head	head;
};
```

@head 链表即用于组织在该等待队列中睡眠的进程


#### wait queue node

wait_queue_entry 抽象等待队列节点，通常每个等待的进程对应一个节点

```c
struct wait_queue_entry {
	unsigned int		flags;
	void			*private;
	wait_queue_func_t	func;
	struct list_head	entry;
};
```

@private 私有数据指针，通常指向对应的睡眠进程的 task_struct

```sh
init_wait/init_wait_entry
    wait->private = current;
    wait->func = autoremove_wake_function;
```

当睡眠进程被唤醒时就会执行 @func 回调函数，通常为 autoremove_wake_function()


### API

等待队列的 API 分为 wait 与 wake 两部分

#### wait_event_*

> wait

```sh
wait_event_*(wq, condition)
```

当前进程期待某个 event 发生，即 @condition 为真，若该 event 未发生，就在等待队列 @wq 上睡眠等待，直到该进程被唤醒；睡眠进程被唤醒时，还需要检查条件 @condition 是否为真，若 @condition 条件为真，则进程退出等待状态，否则该进程将再次进入睡眠状态

```sh
wait_event_*(wq, condition)
    for (;;) {
        if @condition == True: return
    
        set_current_state(@taks_state) // e.g. TASK_INTERRUPTIBLE
        
        @cmd // schedule() by deafault
    }
```


waitqueue 框架实现了一系列 wait_event_*() 接口，以实现不同的功能

1. 等待进程的进程状态 @taks_stat，可以为 TASK_INTERRUPTIBLE 即进程的睡眠状态可以被信号中断、TASK_UNINTERRUPTIBLE 即进程的睡眠状态不可以被信号中断、TASK_KILLABLE 即进程的睡眠状态只能被一些致命信号中断
2. 是否存在超时限制，其中定时功能的实现包括基于 jiffies 的 timer、基于 ktime_t 的 hrtimer 两种
3. 用户可以传入自定义的 @cmd 命令以在进程进入睡眠前执行，用户未指定时默认为 schedule()


函数 | 进程状态 | 超时限制 | 排他性 | 自旋锁保护 | cmd | 返回值
---- | ---- | ---- | ---- | ---- | ---- | ----
wait_event | TASK_UNINTERRUPTIBLE | 无 | 无 | 无 | 无 | 函数返回值为 void，即该函数没有返回值
wait_event_interruptible | TASK_INTERRUPTIBLE (能被信号中断) | 无 | 无 | 无 | 无 | 0, 等待的 event 发生；-ERESTARTSYS, 睡眠过程被信号中断
wait_event_killable | TASK_KILLABLE (只能被一些致命信号中断) | 无 | 无 | 无 | 无 | 0, 等待的 event 发生；-ERESTARTSYS, 睡眠过程被信号中断
wait_event_interruptible_exclusive | TASK_INTERRUPTIBLE | 无 | exclusive | 无 | 无 | 0, 等待的 event 发生；-ERESTARTSYS, 睡眠过程被信号中断
| |
wait_event_timeout | TASK_UNINTERRUPTIBLE | timer，超时时间为 @timeout，以 jiffies 为单位 | 无 | 无 | 无 | 剩余的 jiffies 数（至少为 1）等待的 event 发生；0，超时
wait_event_interruptible_timeout | TASK_INTERRUPTIBLE | timer，超时时间为 timeout，以 jiffies 为单位 | 无 | 无 | 无 | 剩余的 jiffies 数（至少为 1），等待的 event 发生；0，超时；-ERESTARTSYS，睡眠过程被信号中断
| |
wait_event_hrtimeout | TASK_UNINTERRUPTIBLE | hrtimer，超时时间为 timeout，为 ktime_t 数据类型 | 无 | 无 | 无 | 0，等待的 event 发生；-ETIME，超时
wait_event_interruptible_hrtimeout | TASK_INTERRUPTIBLE | hrtimer，超时时间为 timeout，为 ktime_t 数据类型 | 无 | 无 | 无 | 0，等待的 event 发生；-ETIME，超时；-ERESTARTSYS，睡眠过程被信号中断
| |
wait_event_interruptible_locked | TASK_INTERRUPTIBLE | 无 | 无 | 等待队列自身的自旋锁 @wq->lock，spin_lock()/spin_unlock() | 无 | 0, 等待的 event 发生; -ERESTARTSYS, 睡眠过程被信号中断
wait_event_interruptible_locked_irq | TASK_INTERRUPTIBLE | 无 | 无 | 等待队列自身的自旋锁 @wq->lock，spin_lock_irq()/spin_unlock_irq() | 无 | 0, 等待的 event 发生；-ERESTARTSYS, 睡眠过程被信号中断
wait_event_interruptible_exclusive_locked | TASK_INTERRUPTIBLE | 无 | exclusive | 等待队列自身的自旋锁 @wq->lock，spin_lock()/spin_unlock() | 无 | 0, 等待的 event 发生；-ERESTARTSYS, 睡眠过程被信号中断
wait_event_interruptible_exclusive_locked_irq | TASK_INTERRUPTIBLE | 无 | exclusive | 等待队列自身的自旋锁 @wq->lock，spin_lock_irq()/spin_unlock_irq() | 无 | 0, 等待的 event 发生；-ERESTARTSYS, 睡眠过程被信号中断
| |
wait_event_lock_irq | TASK_UNINTERRUPTIBLE | 无 | 无 | 用户自己传入的自旋锁 @lock，spin_lock_irq()/spin_unlock_irq() | 无 | 函数返回值为 void，即该函数没有返回值
wait_event_interruptible_lock_irq | TASK_INTERRUPTIBLE | 无 | 无 | 用户自己传入的自旋锁 @lock，spin_lock_irq()/spin_unlock_irq() | 无 | 0, 等待的 event 发生；-ERESTARTSYS, 睡眠过程被信号中断
wait_event_interruptible_lock_irq_timeout | TASK_INTERRUPTIBLE | 超时时间为 timeout，以 jiffies 为单位 | 无 | 用户自己传入的自旋锁 @lock，spin_lock_irq()/spin_unlock_irq() | 无 | 剩余的 jiffies 数（至少为 1），等待的 event 发生；0，超时；-ERESTARTSYS，睡眠过程被信号中断
| |
wait_event_lock_irq_cmd | TASK_UNINTERRUPTIBLE | 无 | 无 | 用户自己传入的自旋锁 @lock，spin_lock_irq()/spin_unlock_irq() | 用户自己传入的 cmd 命令 | 函数返回值为 void，即该函数没有返回值
wait_event_interruptible_lock_irq_cmd| TASK_INTERRUPTIBLE | 无 | 无 | 用户自己传入的自旋锁 @lock，spin_lock_irq()/spin_unlock_irq() | 用户自己传入的 cmd 命令 | 0, 等待的 event 发生；-ERESTARTSYS, 睡眠过程被信号中断


> Wake

waitqueue 提供一系列 wake_up_*() 接口唤醒在等待队列中睡眠的所有等待进程

其中可以只唤醒等待队列中特定进程状态的等待进程，可以为 TASK_INTERRUPTIBLE 或 TASK_UNINTERRUPTIBLE

```sh
wake_up_*()
    (for each waitqueue entry in the waitqueue)
        ret = entry->func(entry, mode, wake_flags, key), e.g. autoremove_wake_function()
            ret = try_to_wake_up(entry->private, mode, wake_flags)
            if (ret) list_del_init(entry->entry)
        
        if (ret < 0): break
        if (ret && (entry->flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive): break
```

即遍历等待队列中的所有等待节点，调用节点对应的 @func() 回调函数，该回调函数一般实现为 autoremove_wake_function()，其中只是通过 wake_up() 唤醒其对应的睡眠等待的进程


等待节点的 @func() 回调函数的原型如下

```c
typedef int (*wait_queue_func_t)(struct wait_queue_entry *wq_entry, unsigned mode, int flags, void *key);
```

@mode 参数描述当前唤醒操作只唤醒等待队列上进程状态为 mode 的进程，例如 wake_up_interruptible() 中 @mode 为 TASK_INTERRUPTIBLE，即只唤醒 TASK_INTERRUPTIBLE 状态的睡眠进程

@flags 参数会影响唤醒过程中的一些行为，例如 WF_SYNC 标志标明 waker 不能被唤醒的进程抢占

@key 是 wake_up_*() 传入的，给 @func() 回调函数使用的私有数据，一般只有与 @key 匹配的等待进程才会被唤醒，至于 @key 描述什么条件，是由 @func() 回调函数自己解析的


函数的主要工作是唤醒其对应的睡眠等待的进程，返回值大于 0 表明对应的等待进程成功被唤醒 (例如当前等待进程的状态与 @mode 匹配，因而被唤醒)，返回值 0 表明对应的等待进程与当前的唤醒条件不匹配 (例如当前等待进程的状态与 @mode 不匹配)，返回值小于 0 表示唤醒过程中发生了错误，此时 wake_up_*() 必须停止等待节点的遍历并立即返回


函数 | 进程状态 (mode) | nr_exclusive | wake_flags | key | lock-protected
---- | ---- | ---- | ---- | ---- | ----
wake_up | TASK_INTERRUPTIBLE or TASK_UNINTERRUPTIBLE | 1 | 0 | NULL | 无
wake_up_nr | TASK_INTERRUPTIBLE or TASK_UNINTERRUPTIBLE | @nr | 0 | NULL | 无
wake_up_all | TASK_INTERRUPTIBLE or TASK_UNINTERRUPTIBLE | 0 (唤醒所有的排他性进程) | 0 | NULL | 无
| |
wake_up_interruptible | TASK_INTERRUPTIBLE | 1 | 0 | NULL | 无
wake_up_interruptible_nr | TASK_INTERRUPTIBLE | @nr | 0 | NULL | 无
wake_up_interruptible_all | TASK_INTERRUPTIBLE | 0 (唤醒所有的排他性进程) | 0 | NULL | 无
wake_up_interruptible_sync | TASK_INTERRUPTIBLE | WF_SYNC (waker 不能被唤醒的进程抢占) | 0 | NULL | 无
| |
wake_up_locked | TASK_INTERRUPTIBLE or TASK_UNINTERRUPTIBLE | 1 | 0 | NULL | 有 (与 wait_event_*_locked() 这类接口配对使用)
wake_up_all_locked | TASK_INTERRUPTIBLE or TASK_UNINTERRUPTIBLE | 0 (唤醒所有的排他性进程) | 0 | NULL | 有



#### wait_event_*_exclusive

一次 wake_up_\*() 调用默认会唤醒所有通过 wait_event_\*() 在等待队列中睡眠的**所有**进程，但是存在这么一个需求，即调用 wait_event_\*() 的多个进程之间是互斥的，这些进程称为 exclusive 进程，这就要求一次 wake_up_\*() 调用只唤醒其中的一个进程

> wait

此时 wait 端的接口为 wait_event_*_exclusive() 或 prepare_to_wait_exclusive()

```sh
init_wait(entry)
prepare_to_wait_exclusive(wq, entry, TASK_UNINTERRUPTIBLE)
schedule();
finish_wait(wq, entry);
```

其中 prepare_to_wait_exclusive() 将当前进程对应的等待节点 @entry 添加到等待队列 @wq 中，并将当前进程设置为相应状态 (例如 TASK_UNINTERRUPTIBLE)，之后的 schedule() 就会使得当前进程让出调度器，并进入对应的 @state 状态，直到之后被 waker 唤醒，被唤醒之后会调用 finish_wait() 执行相应的清理工作，例如将睡眠进程恢复为 TASK_RUNNING 状态、将等待节点从等待队列中移除

此时等待节点的 @flags 的 WQ_FLAG_EXCLUSIVE 标志表明该进程被唤醒时具有排他性，prepare_to_wait_exclusive() 和 wait_event\_\*\_exclusive_*() 这类接口添加的等待节点都是 exclusive 属性的


> wake

wake() 端的接口为 wake_up_all() 或 wake_up_nr()，其中 @nr_exclusive 描述一次 wake_up_*() 调用允许唤醒的 exclusive 进程的数量

wake_up_*() 执行过程中，最多只能唤醒 @nr_exclusive 个具有 exclusive 属性的等待进程 (可以唤醒的非 exclusive 属性的等待进程的数量不受限制)，@nr_exclusive 为 0 表示可以唤醒的 exclusive 属性的等待进程的数量不受限制，即可以唤醒所有 exclusive 属性的等待进程


#### wait_event_*_locked

之前介绍的 wait_event_*() 接口在判断 @condition 条件的时候，并不会加任何的锁，依赖于调用者自己对 @condition 条件的判断进行保护

```sh
wait_event_*(wq, condition)
    for (;;) {
        if @condition == True: return
    
        set_current_state(@taks_state)
        @cmd
    }
```

但同时 waitqueue 框架也支持 wait_event_*_locked() 这类接口，使用自旋锁对 @condition 条件的检查过程进行保护，其中上锁过程包括 spin_lock() 与 spin_lock_irq() 两种，同时使用的自旋锁又包括等待队列自身的自旋锁以及用户定义的自旋锁两种

```sh
wait_event_*(wq, condition)
    for (;;) {
        # lock
        if @condition == True: return
        # unlock
    
        set_current_state(@taks_state)
        @cmd
    }
```

此时相应地，waker 应该用等待队列自身的自旋锁、或者用户定义的自旋锁对 @condition 状态的变化进行改变，之后调用对应的 wake_up_*_locked() 进行唤醒操作

```sh
# waker
    # lock
    change @condition 
    # unlock
    
    wake_up_locked
```


### wait var/bit

wait 通常的接口为 wait_event(wq, condition)，即当前进程等待 @condition 事件为真，若该事件未发生，就在等待队列 @wq 上睡眠等待，直到该进程被唤醒；睡眠进程被唤醒时，需要再次检查 @condition 事件是否为真，若事件为真，则进程退出等待状态，否则该进程将再次进入睡眠状态

```sh
wait_event_*(wq, condition)
```

注意这里调用者需要自己指定等待队列 @wq，后来 Linux 引入 wait_var() 与 wait_bit() 接口，此时调用者不需要自己指定等待队列，而是由内核自己在预定义的一组等待队列中挑选一个等待队列


```
wait_var_event(var, condition)
wake_up_var(void *var)
```

wait_var_event() 接口描述当前进程等待 @condition 事件 (通常是和 @var 变量有关) 为真 ，若该事件未发生，就根据 @var 变量挑选出一个等待队列，当前进程就在这个等待队列上睡眠等待


```
wait_on_bit(unsigned long *word, int bit, unsigned mode)
wake_up_bit(void *word, int bit)
```

wait_on_bit() 接口描述当前进程等待 @word 标志位的 @bit 位被 clear，若该事件未发生，就根据 @word/@bit 挑选出一个等待队列，当前进程就在这个等待队列上睡眠等待，@mode 为 TASK_INTERRUPTIBLE/TASK_UNINTERRUPTIBLE 等


waitqueue 框架预定义了一组等待队列，即 bit_wait_table[]

```c
static wait_queue_head_t bit_wait_table[WAIT_TABLE_SIZE];
```

wait_var()/wait_bit() 接口就是根据传入的 @var 或者 @word/@bit 经过 hash 计算，在这一组预定义的等待队列中，挑选出一个等待队列，当前进程就在这个等待队列上睡眠等待

```
wait_var_event(var, condition)
    struct wait_queue_head *__wq_head = __var_waitqueue(var);
        return bit_wait_table + hash_ptr(var, WAIT_TABLE_BITS);
    ...
```

```
wait_on_bit(word, bit, mode)
    struct wait_queue_head *wq_head = bit_waitqueue(word, bit);
        val = (unsigned long)word << shift | bit;
        return bit_wait_table + hash_long(val, WAIT_TABLE_BITS);
```


### time sequence

waiter 和 waker 的时序如下所示

- waiter 会先将当前的 wq_entry 添加到 waitqueue 中，之后对传入的 @cond 条件进行检查
- waker 会先将 @cond 条件置为 true，然后在 wakeup 操作中访问 waitqueue，找到对应的 wq_entry

```
CPU0 - waker                    CPU1 - waiter

@cond = true;                   for (;;) {
wake_up(wq_head);                   prepare_to_wait(...);
    # find wq_entry in wq_head list     # add wq_entry into wq_head list
         
                                    if (@cond)
                                        break;
                                    schedule();
                                 }
                                 finish_wait(&wq_head, &wait);
```


> full barrier in waiter side

上述操作中，waiter 在 1) 将 wq_entry 添加到 wq_head 链表，和 2) 检查 @cond 条件，这两个操作之间会使用 full barrier (依赖于 set_current_state() 的 full barrier)，从而防止这两个操作发生乱序

```
CPU0 - waker                    CPU1 - waiter

@cond = true;                   for (;;) {
wake_up(wq_head);                   prepare_to_wait(...);
    # find wq_entry in wq_head list     # add wq_entry into wq_head list
                                        set_current_state()
                                            __smp_mb()
         
                                    if (@cond)
                                        break;
                                    schedule();
                                 }
                                 finish_wait(&wq_head, &wait);
```

否则就有可能发生以下 race，此时 waiter 端 1) 将 wq_entry 添加到 wq_head 链表，和 2) 检查 @cond 条件，这两个操作发生乱序，从而导致 waiter 端会永远失去一次 wakeup

```
CPU0 - waker                    CPU1 - waiter

                                for (;;) {
                                    if (@cond)  <-- @cond is false
                                        break;
                                        
@cond = true;
wake_up(wq_head);                   
    # find no wq_entry in wq_head list     
    
                                    prepare_to_wait(...);
                                        # add wq_entry into wq_head list
    
                                    schedule();  <-- sleep
                                 }
                                 finish_wait(&wq_head, &wait);
```


> full barrier in waker side

那么在 waker 这边一般也需要在 1) 设置 @cond 条件，和 2) 访问 wq_head 链表，这两个操作之间使用 full barrier，从而防止这两个操作发生乱序

```
CPU0 - waker                    CPU1 - waiter

@cond = true;                   for (;;) {
smp_mb()                            prepare_to_wait(...);
if (waitqueue_active(wq_head))         # add wq_entry into wq_head list
    wake_up(wq_head);                  set_current_state()
                                            __smp_mb()
         
                                    if (@cond)
                                        break;
                                    schedule();
                                 }
                                 finish_wait(&wq_head, &wait);
```


否则就有可能发生以下 race，此时 waker 端 1) 设置 @cond 条件，和 2) 访问 wq_head 链表，这两个操作发生乱序，从而导致 waiter 端会永远失去一次 wakeup

```
CPU0 - waker                    CPU1 - waiter

if (waitqueue_active(wq_head)) <-- find no wq_entry in wq_head list
    wake_up(wq_head);

                                for (;;) {
                                   prepare_to_wait(...);
                                        # add wq_entry into wq_head list
                                        set_current_state()
                                            __smp_mb()
         
                                    if (@cond)  <-- @cond is false
                                        break;
                                    schedule(); <-- wq_entry still in
                                                    wq_head list,
                                                    wait for next wakeup
                                 }
                                 finish_wait(&wq_head, &wait);

@cond = true;
```


因而 waker 端在调用 waitqueue_active() 的时候，一定要在 1) 设置 @cond 条件，和 2) 调用 waitqueue_active()，这两个操作之间放置 full barrier


对于 wait_on_bit() 接口来说，可以在 waker 端直接使用 clear_and_wake_up_bit() 接口，该接口是 1) "@cond = true" 与 2) wake_up_bit() 这两个操作的组合

```
# waker
clear_and_wake_up_bit(bit, word)
    clear_bit_unlock(bit, word);
    smp_mb__after_atomic();
    wake_up_bit(word, bit);
```


> implicit full barrier of wake_up()

wake_up() 会在 waitqueue 中寻找匹配的 wq_entry，对于找到的 wq_entry 执行 wakeup 操作

值得注意的是，如果 wake_up() 中找到了至少一个匹配的 wq_entry，那么 wake_up() 自身就隐含有一个 full barrier，此时 waker 端 1) 设置 @cond 条件，和 2) 访问 wq_head 链表，这两个操作就不会发生乱序

否则 wake_up() 自身是没有 full barrier 的

```
CPU0 - waker

@cond = true;

wake_up(wq_head);
    __wake_up
        __wake_up_common
            # iterate waitqueue, for matched wq_entry
            wq_entry->func(), e.g. autoremove_wake_function()
                default_wake_function
                    try_to_wake_up
                        # these two as a whole acts as a full barrier
                        raw_spin_lock_irqsave(&p->pi_lock, ...)
                        smp_mb__after_spinlock()
                    
                    # remove wq_entry from wq_head list
```


因而以下 race 并不会出现，因为一旦 wake_up() 中找到了 wq_entry，wake_up() 就隐含有一个 full barrier，此时 waker 端 1) 设置 @cond 条件，和 2) 访问 wq_head 链表，这两个操作就不会发生乱序

```
CPU0 - waker                    CPU1 - waiter

                                for (;;) {
                                   prepare_to_wait(...);
                                        # add wq_entry into wq_head list
                                        set_current_state()
                                            __smp_mb()

if (waitqueue_active(wq_head))
    wake_up(wq_head);
        # remove wq_entry from wq_head list
         
                                    if (@cond)  <-- @cond is false
                                        break;
                                    schedule(); <-- wq_entry not in
                                                    wq_head list, won't
                                                    be waken forever
                                 }
                                 finish_wait(&wq_head, &wait);

@cond = true;
```


> use waitqueue->lock when full barrier not used

之前说过，即使是调用 wake_up()，在 waker 端 1) 设置 @cond 条件，和 2) 访问 wq_head 链表，这两个操作之间也需要插入 full barrier (因为 wake_up() 在没有唤醒任何东西的时候是不包含 full barrier 的)

如果不使用 full barrier，也可以在 waker 端用 waitqueue->lock 来对 1) 设置 @cond 条件，和 2) 访问 wq_head 链表，这两个操作进行保护

```
CPU0 - waker

spin_lock(wq->lock)
@cond = true;
wake_up_locked(wq_head);                
    # remove wq_entry from wq_head list
spin_unlock(wq->lock)
```

此时可能的时序有

```
CPU0 - waker                    CPU1 - waiter

spin_lock(wq->lock)
wake_up_locked(wq_head); <-- wq_head is empty, no ops
@cond = true;
spin_unlock(wq->lock)
                            # wait_event_interruptible_locked()
                                for (;;) {
                                   prepare_to_wait(...);
                                        spin_lock(wq->lock)
                                        # add wq_entry into wq_head list
                                        spin_unlock(wq->lock)
         
                                    if (@cond)  <-- @cond is true
                                        break;
                                    schedule();
                                 }
                                 finish_wait(&wq_head, &wait);
```

此时 "1) 设置 @cond 条件" 操作就不会重排到 waiter 的 1) 将 wq_entry 添加到 wq_head 链表，和 2) 检查 @cond 条件，这两个操作后面