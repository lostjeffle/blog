title:'NVMe - Control Path'
## NVMe - Control Path


### Init Routine

1. nvme_probe

当检测到 NVMe 设备对应的 PCI 设备时，就会触发执行对应的 nvme_probe 函数，其中会调度 nvme_reset_wq workqueue

```sh
nvme_probe
    queue_work(nvme_reset_wq, &ctrl->reset_work)
```


2. "nvme_reset_wq" worker

"nvme_reset_wq" worker 中会初始化 admin queue，同时分配 IO queue 对应的所有硬件队列，接下来就会调度 nvme_wq workqueue

```sh
"nvme_reset_wq" worker
nvme_reset_work
    nvme_pci_configure_admin_queue // allocate admin hw queue
    nvme_alloc_admin_tags // init admin queue
        blk_mq_alloc_tag_set(&dev->admin_tagset)
        blk_mq_init_queue(&dev->admin_tagset)
    
    nvme_setup_io_queues
        nvme_setup_irqs // allocate MSI IRQ for IO queues
            pci_alloc_irq_vectors_affinity
        nvme_create_io_queues // allocate IO hw queues
    
    nvme_dev_add
        # init dev->tagset, i.e., tagset for IO queues
        blk_mq_alloc_tag_set(&dev->tagset)
        ctrl.tagset = &dev->tagset
    nvme_start_ctrl
        nvme_queue_scan
            queue_work(nvme_wq, &ctrl->scan_work)
```


3. "nvme_wq" worker

"nvme_wq" worker 中会向设备发送 controller identify 命令，返回的 controller identify data structure 中的 Number of Namespaces (NN) 字段描述该设备支持的 namespace 的数量

之后为每个 namespace 创建并注册对应的 request queue

```sh
"nvme_wq" worker
nvme_scan_work
    nvme_scan_ns_sequential
        nvme_identify_ctrl // send 'controller identify' cmd, acquiring 'Number of Namespaces (NN)'
        nvme_validate_or_alloc_ns
            nvme_alloc_ns
                ns->queue = blk_mq_init_queue(ctrl->tagset)
                disk = alloc_disk_node(0, node)
                device_add_disk(..., disk, ...) // register queue
```


### Queue Management

#### data structures

struct nvme_dev 抽象一个 NVMe device

```c
struct nvme_dev;
```


struct nvme_queue 抽象一个硬件队列，@queues[] 数组就管理设备的所有硬件队列，@nr_allocated_queues 描述 @queues[] 数组的长度

```c
struct nvme_dev {
	struct nvme_queue *queues;
	unsigned int nr_allocated_queues;
	...
}
```


struct nvme_ctrl 抽象 NVMe 设备中和 NVMe 协议相关的部分，其中 @queue_count 字段描述当前实际已经分配的硬件队列的数量，也就是当需要新分配一个硬件队列时，实际上就是从 @queues[@queue_count] 获取对应的 struct nvme_queue 结构

```c
struct nvme_dev {
	struct nvme_ctrl ctrl;
	...
}
```

```c
struct nvme_ctrl {
	u32 queue_count;
	...
}
```


@online_queues 描述已经激活的硬件队列的数量，初始化过程中内核完成该硬件队列的初始化之后，就可以将该硬件队列标记为激活状态

```c
struct nvme_dev {
	unsigned online_queues;
	...
}
```


#### number of IO queues

之前介绍过，@nr_allocated_queues 字段描述 @queues[] 数组的长度，这个字段的值一般为 (@write_queue + @poll_queues + possible CPU + 1)，其中

- “1” 也就是 @queues[0] 用于 admin queue
- @queues[] 数组中剩余的，也就是从 @queues[1] 开始的 struct nvme_queue 就用于 IO queue，在内核引入 multi queue mapping 特性之后，
    - @write_queues 参数描述 HCTX_TYPE_DEFAULT 类型的硬件队列数量
    - @poll_queues 参数描述 HCTX_TYPE_POLL 类型的硬件队列数量
    - HCTX_TYPE_READ 类型的硬件队列数量默认为 possible CPU 的数量


所以说每个 namespace 的 IO queue 申请分配的硬件队列数量为
(@write_queue + @poll_queues + possible CPU)

初始化过程中，内核会向设备发送 Set Feature 命令，以请求分配以上数量的硬件队列用于 IO queue；如果设备可以用于 IO queue 的硬件队列数量小于以上申请的硬件队列数量，Set Feature 命令的 completion queue entry 中保存实际分配的用于 IO queue 的硬件队列数量，之后内核只能使用该数量的硬件队列以用于 IO queue

```
nvme_reset_work
    nvme_setup_io_queues
        nvme_set_queue_count
```


#### build queue mapping

为了支持 multi queue mapping 特性，NVMe 驱动引入 @write_queues 与 @poll_queues 两个模块参数，这两个参数的默认值均为 0

- HCTX_TYPE_DEFAULT 类型的硬件队列数量由 @write_queues 参数描述
- HCTX_TYPE_POLL 类型的硬件队列数量由 @poll_queues 参数描述
- HCTX_TYPE_READ 类型的硬件队列数量默认为 possible CPU 的数量

但是以上三种类型的硬件队列数量加起来不能超过设备能够提供的 IO queue 数量（标记为 @nr_io_queues）


```sh
nvme_reset_work
    nvme_setup_io_queues
        nvme_setup_irqs
            dev->io_queues[HCTX_TYPE_POLL] =
            
            @affd.pre_vectors = 1 // allocate IRQ bind to all CPUs for admin queue
            @max_vecs = 1 + nr_io_queues - poll_queues
            pci_alloc_irq_vectors_affinity
                @affd->calc_sets(), i.e., nvme_calc_irq_sets()
                    dev->io_queues[HCTX_TYPE_DEFAULT] =
                    dev->io_queues[HCTX_TYPE_READ] =
```

在调用 pci_alloc_irq_vectors_affinity() 的时候，会同时为 admin queue 与 IRQ IO queue 分配中断，这里 @affd.pre_vectors 是 1，就是说 admin queue 占用 MSI-X 中断 vector 0，这个中断的 CPU affinity 是绑定到所有 possible CPU 上的



三种类型的硬件队列数量的计算规则为

- 首先如果 @poll_queues 大于等于 @nr_io_queues，此时
    - 预留出一个 IO queue 用于 HCTX_TYPE_DEFAULT
    - HCTX_TYPE_READ 的数量为 0
    - 其余的即 (@nr_io_queues - 1) 数量的 IO queue 用于 polling
- 否则 (@nr_io_queues - @poll_queues) 数量的 IO queue 用于 HCTX_TYPE_DEFAULT/HCTX_TYPE_READ，其中
    - 如果未指定 @write_queues 参数，那么
        - 所有剩余的即 (@nr_io_queues - @poll_queues) 数量的 IO queue 全部用于 HCTX_TYPE_DEFAULT
        - HCTX_TYPE_READ 的数量为 0
        - 当然 HCTX_TYPE_POLL 的数量为 @poll_queues
    - 否则如果 @write_queues 大于等于 (@nr_io_queues - @poll_queues)，那么
        - 预留出一个 IO queue 用于 HCTX_TYPE_READ
        - 所有剩余的即 (@nr_io_queues - @poll_queues - 1) 数量的 IO queue 用于 HCTX_TYPE_DEFAULT
        - 当然 HCTX_TYPE_POLL 的数量为 @poll_queues
    - 否则
        - HCTX_TYPE_DEFAULT 的数量即为 @write_queues
        - HCTX_TYPE_READ 的数量为 (@nr_io_queues - @poll_queues - @write_queues)
        - 当然 HCTX_TYPE_POLL 的数量为 @poll_queues


需要注意的是，以上 nvme_setup_irqs() 完成后，为各个 queue map 中的硬件队列分配中断，同时各个 queue map 实际分配的硬件队列数量保存在 nvme_dev.io_queues[] 数组中
```c
struct nvme_dev {
	unsigned io_queues[HCTX_MAX_TYPES];
	...
}
```


之前描述过，此时 nvme_dev.io_queues[] 数组中就保存了各个 queue map 实际分配的硬件队列的数量；之后为 request queue 分配 tagset 的时候，就会根据 io_queues[HCTX_TYPE_POLL] 设置是否需要 polling queue map

```sh
nvme_reset_work
    nvme_dev_add
        dev->tagset.nr_maps = 2; /* default + read */
        if (dev->io_queues[HCTX_TYPE_POLL])
            dev->tagset.nr_maps++;
```


之后在初始化 tagset 的过程中，就会调用 mq_ops->map_queues() 设置 per-cpu ctx -> hctx 的映射关系，对于 NVMe 来说就是根据硬件队列的 MSI 中断的 CPU affinity，设置该硬件队列映射到哪些软件队列

```sh
nvme_reset_work
    nvme_dev_add
        blk_mq_alloc_tag_set
            blk_mq_update_queue_map
                mq_ops->map_queues(), that is, nvme_pci_map_queues
                    (for each type mapping)
                        map->nr_queues = dev->io_queues[i]
                        map->queue_offset = ...
                        blk_mq_pci_map_queues()/blk_mq_map_queues()
```

- 对于 HCTX_TYPE_POLL，是按照 default CPU topology 映射的
- 对于 HCTX_TYPE_DEFAULT/HCTX_TYPE_READ，是按照这些硬件队列的 MSI-X 中断的 CPU affinity 映射的


### RESET

```sh
# NVME_IOCTL_RESET ioctl on /dev/nvme0
nvme_dev_ioctl
    nvme_reset_ctrl_sync
        nvme_reset_ctrl
            # queue reset_work into nvme_reset_wq

nvme_reset_work
    // recalculate nvme_dev.io_queues[]， considering @nr_io_queues read from 'Set Feature' command, @write_queues/@poll_queues param
    nvme_setup_io_queues
    
    nvme_dev_add
        blk_mq_update_nr_hw_queues(tagset, new_nr_io_queues)
            mq_ops->map_queues(), that is, nvme_pci_map_queues
    
    nvme_start_ctrl
        nvme_queue_scan
            queue_work(nvme_wq, &ctrl->scan_work)
```

需要注意的是，发生 RESET 之后会根据 Set Feature 命令返回的硬件设备支持的 IO 队列的数量、poll_queues/write_queues 模块参数的值，重新计算 nvme_dev.io_queues[] 数组，即各个 queue map 中硬件队列的数量

如果 RESET 前后 poll_queues 模块参数的值发生变化，那么 nvme_dev.io_queues[HCTX_TYPE_POLL] 的值就会发生变化，因而 RESET 前后设备是否支持 polling 可能是会变化的



```sh
"nvme_wq" worker
nvme_scan_work
    nvme_scan_ns_sequential
        nvme_identify_ctrl // send 'controller identify' cmd, acquiring 'Number of Namespaces (NN)'
        nvme_validate_or_alloc_ns
            nvme_validate_ns
                nvme_update_ns_info // update queue info
```
