## VFS Lifecycle - 2 Filesystem


### Object Refcount

#### mount

```c
struct mount {
	struct mnt_pcp __percpu *mnt_pcp;
	...
}

struct mnt_pcp {
	int mnt_count;
	...
};
```

@mnt_pcp.mnt_count per-CPU 引用计数用于实现 mount 的生命周期管理


> init

mount 创建时 @mnt_pcp.mnt_count 计数的初始值为 1

```
ksys_mount
    do_mount
        do_new_mount
            vfs_kern_mount
                alloc_vfsmnt
                    this_cpu_add(mnt->mnt_pcp->mnt_count, 1)
```

最后在卸载文件系统的时候，会减小对应的 mount 的 @mnt_pcp.mnt_count 计数

```
ksys_umount
    mntput_no_expire
        mnt_add_count(, -1)
```


> mount/umount

在将一个文件系统挂载到某个 mntpoint 的时候，会增加该 mntpoint 所在的 mount 的 @mnt_pcp.mnt_count 计数

```
ksys_mount
    do_mount
        do_new_mount
            do_add_mount
                graft_tree
                    attach_recursive_mnt
                        mnt_set_mountpoint
                            mnt_add_count(, 1)
```

在卸载某个文件系统的时候，会减小其 mntpoint 所在的 mount 的 @mnt_pcp.mnt_count 计数

```
ksys_umount
    do_umount
        umount_tree
            mnt_add_count(, -1)
```


> open/close file

文件系统挂载之后，对这个文件系统上的一个文件执行 open 操作，会增加该文件系统的 mount 的 @mnt_pcp.mnt_count 计数

```
do_sys_open
    do_filp_open
        path_openat
            path_init
                path_get
                    mntget
```

对文件执行 close 操作，会减少该文件系统的 mount 的 @mnt_pcp.mnt_count 计数

```
sys_close
    __close_fd
        filp_close
            fput
                mntput
                    mntput_no_expire
```

因而当文件系统上的一个文件正被进程 open 占用着的时候，该文件系统不能执行 umount 操作，否则 umount 操作会返回 -EBUSY 错误


> cleanup

此时如果 mount 的引用计数已经减小为 0，那么就会调用 cleanup_mnt()

```
ksys_umount
    mntput_no_expire
        mnt_add_count(, -1)
        cleanup_mnt
            deactivate_super
            call_rcu(, delayed_free_vfsmnt)
```

这里主要是调用 deactivate_super()，最后会调用 delayed_free_vfsmnt() 释放 mount 结构


#### superblock

**external/internal reference count**

reference count 有时也可分为 external (strong) reference count 与 internal (weak) reference count 两类


internal 计数其实就是我们平时最常说的引用计数，描述当前存在多少实例**引用**该 object，如果这个 object 是动态内存分配的，那么只要该计数不为 0 那么就不能释放这个 object

对于 superblock 来说，@s_count 计数就表示 internal 计数，当这个计数变为 0 时就会释放 superblock 占用的内存


而 external 表示当前存在其他实例正在**使用**该 object

对于 superblock 来说，@s_active 计数就表示 external 计数，表示当前有多少 mount 正在使用该 superblock，当这个计数变为 0 时需要对这个 superblock 对应的文件系统执行 sync 操作，即将该文件系统中所有文件的 dirty page 回写到磁盘中


@s_active 计数主要是由 superblock 对应的 mount 实例维护，superblock 的每个 mount 实例都会增加 superblock 的 @s_active 计数

而对于 @s_count 计数，除了每个 mount 实例会增加 superblock 的 @s_count 计数之外，每次访问 superblock 时需要增加 @s_count 计数，以防止该 superblock object 被释放，在访问结束后再减小 @s_count 计数


##### reference counting

```c
struct super_block {
	int			s_count;
	...
}
```

@s_count 计数用于实现 superblock 的生命周期管理，通过 get_super()/put_super() 增加、减小 superblock 的 @s_count 计数


> mount/umount

文件系统挂载过程中，只有该文件系统的第一个 mount 挂载的时候，也就是 superblock 刚创建时 @s_count 计数的初始值为 1，之后其他的 mount 挂载时，不会增加 @s_count 计数

```
ksys_mount
    do_mount
        do_new_mount
            vfs_kern_mount
                mount_fs
                    .mount(), that is, ext4_mount
                        mount_bdev
                            sget
                                alloc_super
                                    s->s_count = 1
```


文件系统卸载过程中，如果该文件系统的最后一个 mount 也被卸载时（即 active 计数变为 0 时），最终会减小 superblock 的 @s_count 计数

```
ksys_umount
    mntput_no_expire
        cleanup_mnt
            deactivate_super
                atomic_add_unless(s_active, -1, 1)
                    deactivate_locked_super
                        atomic_dec_and_test(s_active)
                            put_super(s)
                                --s->s_count
```

> cleanup

这一过程中当 @s_count 计数减为 0 时，就会执行相应的清理操作，其中主要是调用 destroy_super_rcu() 释放 super_block 结构

```
put_super
    --s->s_count
        call_rcu(, destroy_super_rcu)
```


##### active counting

```c
struct super_block {
	atomic_t		s_active;
	...
}
```

@s_active 计数用于描述当前有多少该 superblock 对应的 mount 实例，通过 grab_super()/deactivate_super() 增加、减小 superblock 的 @s_active 计数


> init

在文件系统挂载过程中，superblock 刚创建时 @s_active 计数的初始值为 1

```
ksys_mount
    do_mount
        do_new_mount
            vfs_kern_mount
                mount_fs
                    .mount(), that is, ext4_mount
                        mount_bdev
                            sget
                                alloc_super
                                    atomic_set(s_active, 1)
```

> mount/umount

在文件系统挂载过程中，会增加对应的 superblock 的 @s_active 计数

```
ksys_mount
    do_mount
        do_new_mount
            vfs_kern_mount
                mount_fs
                    .mount(), that is, ext4_mount
                        mount_bdev
                            sget
                                grab_super
                                    atomic_inc_not_zero(s_active)
```

在文件系统卸载过程中，会减小对应的 superblock 的 @s_active 计数

```
ksys_umount
    mntput_no_expire
        cleanup_mnt
            deactivate_super
                atomic_add_unless(s_active, -1, 1)
```

> clone mount

我们知道每个 mount instance 都用一个对应的 struct mount 结构，bind mount 也不例外；在 bind mount 路径中，例如 "mount --bind dir1 dir2" 将 dir1 挂载到 dir2 过程中，创建的 struct mount 结构，实际上是 dir1 所在的文件系统的 struct mount 的拷贝，这一过程是通过 clone_mnt() 完成的，其中会增加 dir1 所在的文件系统的 superblock 的 @s_active 计数

因而如果将 dir1 bind 挂载到 dir2，那么即使 dir1 所在的文件系统已经卸载，但是由于 dir2 所在的 bind mount 仍然没有卸载，因而此时 dir1 所在的文件系统对应的 superblock 的 @s_active 计数仍不为 0，因而此时该 superblock 还未执行清理操作

```
clone_mnt
    atomic_inc(&sb->s_active)
```

类似地，overlayfs 中，对于 lowerdir、upperdir 都会执行 clone_mnt 操作，也就是增加这些 lowerdir、upperdir 所在的文件系统对应的 superblock 的 @s_active 计数


> cleanup

此时如果 superblock 的 @s_active 计数变为 0，就会调用 deactivate_locked_super() 执行相应的清理操作

```
deactivate_super
    atomic_add_unless(s_active, -1, 1)
        deactivate_locked_super
            atomic_dec_and_test(s_active)
                .kill_sb(), that is, kill_block_super
                put_super(s)
```

其中主要调用 fs->kill_sb() 回调函数执行该文件系统的清理操作，最后会减小该 superblock 的 @s_count 计数


### Cleanup Routine

umount
*mount->mnt_count --*
if (mnt_count == 0) **cleanup_mnt**
　|- **deactivate_super**
　|　|- *superblock->s_active --*
　|　|- if (s_active == 0)
　|　|　|- .kill_sb(), that is, **kill_block_super**
　|　|　　|- **generic_shutdown_super**
　|　|　　|- **sync_blockdev**
　|　|　　|- *blockdev->bd_openers --*
　|　|　　|- (blockdev->bd_openers  == 0)
　|　|　　　|- **sync_blockdev**
　|　|　　　|- **kill_bdev**
　|　|　　　|- **bdev_write_inode**
　|　|　　　|- *blockdev->bd_inode->i_count --*
　|　|　　　|- (blockdev->bd_inode->i_count  == 0)
　|　|　　　　|- **iput_final** (free inode+blockdev)
　|　|　
　|　|- *superblock->s_count --*
　|　|- (s_count == 0) destroy_super_rcu (free superblock)
　|
　|- delayed_free_vfsmnt (free mount)



#### umount

首先看一下 umount 路径中，当一个 superblock 对应的文件系统的所有挂载点都已经卸载时（即 @s_active 计数变为 0），就会对该文件系统执行清理操作

```
s_op->kill_sb(), e.g. kill_block_super()
    generic_shutdown_super
        shrink_dcache_for_umount
        sync_filesystem(sb)
        evict_inodes(sb)
        # if (!list_empty(&sb->s_inodes)), "VFS: Busy inodes after unmount. Self-destruct in 5 seconds.  Have a nice day..."
    sync_blockdev
    blkdev_put
```

- shrink_dcache_for_umount() 释放 dcache
- sync_filesystem() 回写该文件系统中的所有 dirty inode 的数据 (page cache) 与元数据 (blockdev)
- evict_inodes() 遍历 sb->s_inodes 链表中的所有 inode，对于 @i_count 计数已经变为 0 的 inode 调用 evict(inode) 以释放该 inode

最后 sync_blockdev() 中对该文件系统所在的 blockdev 的 bdev inode 执行回写操作，实际上是确保该文件系统的所有元数据回写完成


之后当一个 blockdev 的 @bd_openers 计数变为 0 时，就会对该 blockdev 执行清理操作

```
fs_type->kill_sb(), e.g. kill_block_super()
    generic_shutdown_super
    sync_blockdev
    
    blkdev_put
        # when bd_openers == 0
        blkdev_flush_mapping
            sync_blockdev
            kill_bdev
                truncate_inode_pages(bdev->bd_inode->i_mapping)
            bdev_write_inode
```

- sync_blockdev() 对该 blockdev 的 bdev inode 执行回写操作，确保该 blockdev 的所有 page cache 都回写到 blockdev
- truncate_inode_pages() 清空 blockdev 的 bdev inode 的 address space，其中会等待 page cache 回写完成，并释放这些 page cache（通过减小 page 的引用计数）
- bdev_write_inode() 再次对该 blockdev 的 bdev inode 执行回写操作


#### superblock init/cleanup

 > init

mount routine 中会调用 parse_param() 回调函数解析挂载参数，之后调用 get_tree() 回调函数解析 superblock、并执行文件系统特定的初始化操作

```sh
# mount(2)
do_new_mount
    fc = fs_context_for_mount(type, ...)
        # allocate 'struct fs_context' @fc
        fs_type->init_fs_context(fc)
            # allocate filesystem specific fs_context
            fc->fs_private = filesystem specific fs_contex
            fc->ops = filesystem specific fs_context_operations
    
    parse_monolithic_mount_data
        generic_parse_monolithic
            vfs_parse_fs_string
                vfs_parse_fs_param
                    fs_type->parse_param()
                        # parse mount options
        
    vfs_get_tree(fc)
        fc->ops->get_tree(fc), e.g. get_tree_bdev()/get_tree_nodev()
            XXX_fill_super
                # filesystem specific initialization
```


> cleanup

fs_type 的 kill_sb() 回调函数用于对 superblock 执行清理操作

```c
struct file_system_type {
	void (*kill_sb) (struct super_block *);
	...
}
```

其中还会调用到 s_op->put_super() 回调函数

```
fs_type->kill_sb()
    generic_shutdown_super
        shrink_dcache_for_umount
        sync_filesystem(sb)
        evict_inodes(sb)
        
        if (sb->s_root):
            s_op->put_super()
        
        if (!list_empty(&sb->s_inodes)):
            "VFS: Busy inodes after unmount..."
```


有两处地方会调用到 fs_type->kill_sb() 回调函数

一个是 mount routine 初始化阶段，初始化失败的时候就会触发执行 fs_type->kill_sb() 回调函数，由于此时初始化操作可能是执行到中间某个步骤从而触发执行的 kill_sb() 回调函数，因而 kill_sb() 回调函数必须能够处理这种尚未初始化完成情况下的清理操作

```sh
# mount(2)
do_new_mount
    # on failure
    put_fs_context
        sb = fc->root->d_sb
        deactivate_super(sb)
            # when superblock's @s_active refcount == 0
            fs_type->kill_sb()
```


另一种就是 umount routine 中，当 superblock 的 @s_active 计数变为 0，即该 superblock 对应的所有挂载点都已经卸载时，就会触发执行 fs_type->kill_sb() 回调函数

```
# when mount's @mnt_count refcount == 0
cleanup_mnt
	deactivate_locked_super
        # when superblock's @s_active refcount == 0
        fs_type->kill_sb()
```


> kill_sb()/put_super()

这里需要注意的是，kill_sb()/put_super() 这两个回调函数中都可以执行清理操作，两者的关系是，在 kill_sb() 回调函数执行到中间的时候，会插入执行 put_super() 回调函数

```
fs_type->kill_sb()
    # cleanup in kill_sb()
    
    generic_shutdown_super
        if (sb->s_root):
            s_op->put_super()
        
        if (!list_empty(&sb->s_inodes)):
            "VFS: Busy inodes after unmount..."

    # cleanup in kill_sb()
```

同时需要注意的是，只有 sb->s_root 字段不为空的时候，才会执行 put_super() 回调函数

而 sb->s_root 字段是在文件系统初始化阶段赋值的，sb->s_root 字段的赋值操作同样把文件系统的初始化操作划分为两个部分

```sh
# mount(2)
do_new_mount
    vfs_get_tree(fc)
        fc->ops->get_tree(fc), e.g. get_tree_bdev()/get_tree_nodev()
            XXX_fill_super
                # filesystem specific initialization A
                
                sb->s_root = d_make_root(inode)
                
                # filesystem specific initialization B
```

因而 put_super() 回调函数通常负责清理上述 initialization B 部分的清理操作，上述 initialization A 部分的清理操作一定不能放在 put_super() 回调函数中，而通常放在 kill_sb() 回调函数中
