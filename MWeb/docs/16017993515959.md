title:'IO - 4 Direct IO - iomap'
## IO - 4 Direct IO - iomap


### Background

open() 系统调用中传入的 @flag 参数设置上 O_DIRECT 标志，那么接下来对该文件的 IO 操作就是 Direct IO

```
open() syscall: @flag |= O_DIRECT 
    file->f_flags |= O_DIRECT
        
read()/write() syscall
    vfs_read/vfs_write
        new_sync_read.new_sync_write
            iocb->ki_flags |= IOCB_DIRECT
            f_op->read_iter()/write_iter()
```


#### enhanced filemap-based DIRECT IO

在 iomap 框架支持 direct IO 之前，filemap-based direct IO 路径经过**增强**，已经支持 extent-based mapping，此时也是以 extent 为单位进行地址翻译的

```sh
__blockdev_direct_IO
    do_blockdev_direct_IO
        do_direct_IO
            # divide at page boundary, for each page
                if sdio->blocks_available == 0:
                    get_more_blocks
                        # call get_block() to do address translation
                    
                    # length of extent stored in @sdio->blocks_available
                else:
                    # reuse the previous address translation result
```

这里的增强之处主要在于，在 @blocks_available 字段缓存上次执行地址翻译的 extent 的长度，这样映射到同一个 extent 的不同 page 都可以复用上一次的地址翻译的结果，从而使得一个 extent 执行一次地址翻译

```c
struct dio_submit {
	unsigned blocks_available;	/* At block_in_file.  changes */
	...
}
```


#### iomap-based DIRECT IO

iomap 框架在 v4.10 的 commit ff6a9292e6f6 ("iomap: implement direct I/O")，引入 iomap_dio_rw() 函数来支持 iomap-based direct IO

```c
ssize_t
iomap_dio_rw(struct kiocb *iocb, struct iov_iter *iter,
		const struct iomap_ops *ops, const struct iomap_dio_ops *dops,
		bool wait_for_completion)
```

其中根据传入的 kiocb、iov_iter 以及 iomap 框架映射得到的 iomap 创建并初始化对应的 bio，最终调用 submit_bio() 下发该 bio

```sh
.read_iter()/.write_iter()
    # for DIRECT IO
    iomap_dio_rw
```

v5.5 版本 ext4 的 direct IO 切换到 iomap 框架


此外需要注意的是，之前介绍过传统的 filemap DIRECT IO 依赖于 a_ops->direct_IO() 回调函数来实现 DIRECT IO，这主要是因为其中涉及到地址映射部分的操作，是各个文件系统相关的 (get_block())，因而通过各个文件系统定义各自的 a_ops->direct_IO() 回调函数来实现

```sh
.read_iter(), e.g. generic_file_read_iter()
.write_iter(), e.g. generic_file_write_iter()
    # for DIRECT IO
    a_ops->direct_IO()
```

但是 iomap DIRECT IO 中，各个文件系统相关的地址映射，则是由文件系统各自定义的 struct iomap_ops 抽象，因而各个文件系统的 .read_iter()/.write_iter() 都是直接调用 iomap_dio_rw()，此时 a_ops->direct_IO() 被弃用

但是 VFS 部分代码又依赖于，根据 a_ops->direct_IO() 回调函数是否定义，来判断对应的文件是否支持 DIRECT IO

```
do_dentry_open
    if (f->f_flags & O_DIRECT):
        if (!a_ops->direct_IO):
            return -EINVAL;
```

因而支持 iomap DIRECT IO 的文件系统往往还是需要定义一个 noop a_ops->direct_IO() 回调函数，其往往实现为 noop_direct_IO()


### Submit Routine

#### iomap_dio

Direct IO 路径中会为每次 direct IO 分配一个对应的 iomap_dio 结构，该结构贯穿了整个 direct IO 路径

> kiocb/iov_iter

```c
struct iomap_dio {
	struct kiocb		*iocb;
	...

	union {
		/* used during submission and for synchronous completion: */
		struct {
			struct iov_iter	*iter;
			...
		} submit;
		...
	};
};
```

@iocb/@iter 就是 direct IO 传入的 kiocb 与 iov_iter


> ref

一次 direct IO 操作的数据可能需要下发多个 bio，即一个 iomap_dio 对应多个 bio 结构，@ref 计数就用于追踪其对应的多个 bio 的完成情况，其初始值为 1，每个 bio 都会增加 @ref 计数

```c
struct iomap_dio {
	atomic_t		ref;
	...
};
```


> error

@error 字段描述这次 direct IO 整体的执行情况，之前描述过一次 direct IO 过程中可能下发多个 bio，因而每个 bio 完成即调用 bio->bi_end_io() 时都会检查并设置 iomap_dio->error 字段

bio->bi_status 字段描述当前完成的 bio 的执行情况，只有当这个 iomap_dio 对应的所有 bio 都成功执行时，iomap_dio->error 字段才为 0，否则该字段就保存有其中一个失败的 bio 的错误码

```c
struct iomap_dio {
	int			error;
	...
};
```


#### submit routine

iomap_dio_rw() 会循环处理当前执行 IO 的文件偏移范围内的所有 extent，对于其中的每个 extent 调用 iomap_dio_actor() 进行处理，此时每个 extent 就会发送一个 bio

```sh
iomap_dio_rw

	do {
	   iomap_apply(..., iomap_dio_actor);
	} while ((count = iov_iter_count(iter)) > 0);
```


iomap_dio_actor() 中就会将当前 extent 中的数据封装为 bio 进行下发

由于单个 bio 最多只能包含 256 个 bvec，如果当前 extent 的数据超过了 256 个 bvec，那么当前这个 extent 就会拆分为多个 bio 进行下发

```sh
iomap_apply
    iomap_dio_actor
        iomap_dio_bio_actor
        
            	do {
            	   ret = bio_iov_iter_get_pages(bio, dio->submit.iter);
            	   nr_pages = iov_iter_npages(dio->submit.iter, BIO_MAX_PAGES);
            	   # alloc and init bio
            	   iomap_dio_submit_bio(dio, iomap, bio, pos);
            	       dio->submit.cookie = submit_bio(bio)
            	} while (nr_pages);
            	
            	iov_iter_reexpand
```


### Complet Routine

sync/async IO 的 completion routien 各有不同，按照以下标准区分 sync/async IO

- 如果 kiocb->ki_complete 不为 NULL，那么就为 async IO，因而 libaio/io_uring 这些异步框架都会设置该字段
- 否则就是 sync IO


#### async routine

> return -EIOCBQUEUED

对于 async direct IO，传入的 kiocb->ki_complete() 通常会被设置为对应的异步框架下的回调函数，此时 iomap_dio_rw() 传入的 @wait_for_completion 参数就为 0，因而 iomap_dio_rw() 中就会直接返回 -EIOCBQUEUED，而不会睡眠等待这次 direct IO 的完成

```c
iomap_dio_rw()

	if (!atomic_dec_and_test(&dio->ref)) {
		if (!wait_for_completion)
			return -EIOCBQUEUED;

		for (;;) {
			...
		}
	}
```


> call kiocb->ki_complete() in bio completion routine

在返回 -EIOCBQUEUED 之前，会减去 @iomap_dio->ref 的 initial refcount 计数

同时 iomap DIRECT IO 中 bio 的 @bi_end_io 回调函数均为 iomap_dio_bio_end_io()，其中会减小对应的 @iomap_dio->ref 计数；如果这个计数减为 0，即这次 direct IO 过程中下发的所有 bio 都已经完成，那么最终就会调用这次 kiocb 的 @ki_complete() 回调函数

```sh
bio->bi_end_io(), that is, iomap_dio_bio_end_io()
    if (atomic_dec_and_test(&dio->ref)):
        iomap_dio_complete_work
            iocb->ki_complete()
```


#### sync routine

> sleep or poll waiting for all bios completed

对于 sync Direct IO，iomap_dio->wait_for_completion 字段设置为 1

在减去 @iomap_dio->ref 的 initial refcount 之后，就会调用 blk_io_schedule() 在当前进程上下文中等待这次 direct IO 过程中下发的所有 bio 完成

对于 sync HIPRI IO 来说，还会在当前进程上下文中执行 polling 操作


```c
iomap_dio_rw()
    dio->submit.waiter = current;
    ...
    if (!atomic_dec_and_test(&dio->ref)):
    		for (;;) {
    			if (!READ_ONCE(dio->submit.waiter))
    				break;
    				
    			if (!(iocb->ki_flags & IOCB_HIPRI) ||
			    !dio->submit.last_queue ||
			    !blk_poll(dio->submit.last_queue,
					 dio->submit.cookie, true))
				blk_io_schedule();
    		}
		}
```


> wakeup in bio completion routine

iomap DIRECT IO 中 bio 的 @bi_end_io 回调函数均为 iomap_dio_bio_end_io()，其中会减小对应的 dio->ref 计数；如果这个计数减为 0，即这次 direct IO 过程中下发的所有 bio 都已经完成，那么就会唤醒之前睡眠等待的进程

```sh
bio->bi_end_io(), that is, iomap_dio_bio_end_io()
    	if (atomic_dec_and_test(&dio->ref)) {
			struct task_struct *waiter = dio->submit.waiter;
			WRITE_ONCE(dio->submit.waiter, NULL);
			blk_wake_io_task(waiter);
	}
```


> check direct IO completed

这里需要注意的是，一次 direct IO 全部完成的判断标准是 iomap_dio->submit.waiter 字段为 NULL

- 在下发 IO 之前，会设置 iomap_dio->submit.waiter 字段为 current 即当前进程
- 进程在睡眠等待的过程中，正是通过 iomap_dio->submit.waiter 字段的值是否为 NULL 来判断其等待的 direct IO 是否已经全部执行完成的
- 最后在 bio completion 路径中，当一次 direct IO 过程中下发的所有 bio 都已经完成时，就会将 iomap_dio->submit.waiter 字段重新清为 NULL，以标志当前 direct IO 已经全部完成


### short read

当 DIRECT IO 执行完成的时候，会返回实际成功读取/写入的数据量

我们知道传入的 @iter->count 描述了该 DIRECT IO 需要执行的数据量之和

```c
ssize_t
iomap_dio_rw(struct kiocb *iocb, struct iov_iter *iter,
		const struct iomap_ops *ops, const struct iomap_dio_ops *dops,
		bool wait_for_completion)
```

对于 sync DIRECT IO 来说，iomap_dio_rw() 中会同步等待 IO 执行完成，因而 iomap_dio_rw() 的返回值就描述了成功访问的数据量；发生错误的时候会返回负的错误码 (包含 0)

而对于 async DIRECT IO 来说，iomap_dio_rw() 中下发 IO 之后就会立即返回 -EIOCBQUEUED，当发生其他错误的时候会返回其他负的错误码 (包含 0)；而之后当 IO 真正执行完成的时候就会调用 @iocb->ki_complete() 回调函数

```c
void (*ki_complete)(struct kiocb *iocb, long ret, long ret2);
```

其中的 @ret 参数就描述了成功访问的数据量；发生错误的时候是负的错误码


那么 iomap_dio_rw() 是否会发生 partial complete，也就是最终成功访问的数据量小于最初请求访问的数据量 (@iter->count) 呢？


首先 iomap_dio_rw() 中会下发所有请求访问的数据量，即将 @iter->count 范围内的所有数据封装为对应的 bio 并下发

这一过程中需要执行 pin page 操作，这一过程中可能有部分 page 的 pin 操作会失败而提前返回，此时 __iomap_dio_rw() 中会将错误码保存在 @iomap_dio->error 字段，这样就会导致整个 iomap_dio_rw() 返回错误

```sh
iomap_dio_rw
    __iomap_dio_rw
        iomap_dio_iter
            iomap_dio_bio_iter
                do {
                    ret = bio_iov_iter_get_pages() // pin page
                    if (ret)
                        # return ...
                    
                    iomap_dio_submit_bio // submit bio
                } while (nr_pages)
    
        ...
        
        iomap_dio_set_error
```

以上描述了 iomap_dio_rw() 阶段不会发生 partial submit，即 @iter->count 范围内的所有数据都会封装为对应的 bio 并下发

而对于 MQ block 来说，request 不存在 partial complete 的情况，因而一个 request 中的所有 bio 要么全部成功完成，要么全部失败

因而这就确保了 iomap_dio_rw() 不会发生 partial complete，即没有发生错误的时候，其最终成功访问的数据量总是等于最初请求访问的数据量 (@iter->count)



iomap_dio_rw() 是有可能返回 0 的，例如传入的 @iter->count 为 0 的时候就会返回 0；当传入的 @iocb->ki_pos 一开始就超过 @i_size，即 READ IO 一开始就尝试读取超过文件大小的数据时，也会返回 0

```sh
iomap_dio_rw
    __iomap_dio_rw
        iomi.pos = iocb->ki_pos
        dio->i_size = i_size_read(inode)
        if omi.pos >= dio->i_size:
            return NULL
    return 0
```

当传入的 @iocb->ki_pos 一开始没有超过 @i_size，但是 (@iocb->ki_pos + iter->count) 超过了 @i_size，即读到后面发生了 EOF，那么 iomap_dio_rw() 实际上返回的是 (@i_size - @iocb->ki_pos)，即去除超过 EOF 的那一部分

```sh
iomap_dio_rw
    __iomap_dio_rw
        iomi.pos = iocb->ki_pos
        dio->i_size = i_size_read(inode)
        iomap_dio_iter
            # submit bio
            dio->size += iomap_length(iter) # e.g. one block size though i_size < one block size

    iomap_dio_complete
        offset = iocb->ki_pos
        if offset + dio->size > dio->i_size && it's READ IO:
            ret = dio->i_size - offset;
        iocb->ki_pos += ret
        return ret
```

因而当传入的 iocb 超过 EOF 的时候，iomap_dio_rw() 的返回值是有可能小于最初请求访问的数据量 (@iter->count)；否则其返回值总是等于最初请求访问的数据量 (@iter->count)


### Alignment

#### align with bdev logical block size

块设备的 logical_block_size 描述硬件进行地址寻址的最小单元，其默认值为 512 bytes，对应 `/sys/block/<dev>/queue/logical_block_size`；向块设备下发的 IO 必须按照 logical_block_size 对齐

因而在 DIRECT IO 中，以下参数必须按照 logical_block_size 对齐

- user buffer 的起始地址 (iter->iov[k].iov_base) 和长度 (iter->iov[k].iov_len)
- 需要传输的数据量 (iter->count)
- 需要访问的文件位置 (iocb->ki_pos)

如果以上任一参数不满足要求，DIRECT IO 就会返回 -EINVAL

```sh
iomap_dio_rw
    __iomap_dio_rw
        iomap_dio_iter
            iomap_dio_bio_iter
                if ((pos | length | align) & ((1 << blkbits) - 1)):
		              return -EINVAL;
```


#### zero padding with DIRECT write

DIRECT write 过程中，如果文件写入位置处尚未分配磁盘空间，那么 DIRECT IO 过程中在真正写入之前会预分配相应的磁盘空间，例如 ext4 中就会预分配 unwritten extent

```sh
iomap_dio_rw
    __iomap_dio_rw
        iomap_iter
            ops->iomap_begin, e.g. ext4_iomap_begin
                if (flags & IOMAP_WRITE): ext4_iomap_alloc
                    # allocate unwritten extent
                    ext4_map_blocks(..., EXT4_GET_BLOCKS_IO_CREATE_EXT)

        iomap_dio_iter
            iomap_dio_bio_iter
                if (iomap->type == IOMAP_UNWRITTEN): need_zeroout = true
                if (need_zeroout):
                    # zero heading
                    # zero tail
```


由于文件系统实现地址映射的最小单元是 fs block size，因而当 DIRECT write 写入的数据量小于 fs block size 的时候 (partially write)，这个 fs block 整体会由 unwritten extent 转换为 written extent，同时这个 fs block 中未写入的部分会做 zero padding 处理

```
                    fs block size
<------------------------------------------------------>

+--------------+------------------------+--------------+
| zero padding |    written content     | zero padding |
+--------------+------------------------+--------------+
               <------------------------>
               ^       iter->count
        iocb->ki_pos
```