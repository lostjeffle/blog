title:'Backend - vhost-user'
## Backend - vhost-user


### Control Routine - virtiofsd

virtiofsd 使用一个 UNIX socket 来实现与 qemu 之间的通讯，用户可以通过 "--socket-path" 参数指定这个 UNIX socket 的路径，这个 UNIX socket 对应的 fd 就保存在 @vu_socketfd 中

```c
struct fuse_session {
    int   vu_socketfd;
    ...
}
```


同时 virtiofsd 会初始化相应的数据结构

```sh
main
    fuse_session_mount
        virtio_session_mount
            vu_init
                dev->sock = @vu_socketfd
                dev->panic = fv_panic
                dev->read_msg = vu_message_read_default;
                dev->set_watch = fv_set_watch
                dev->remove_watch = fv_remove_watch
                dev->iface = fv_iface
```


之后 virtiofsd 就会进入循环，以等待 qemu 的启动

```sh
main
    virtio_loop
        vu_dispatch
            dev->read_msg(), i.e., vu_message_read_default()
            vu_process_message
```


### Control Routine - qemu

qemu 中通过 "-device vhost-user-fs-pci, ..." 参数来指定 virtiofs 设备，在初始化过程中 qemu 就会通过 vhost-user 协议向 virtiofsd 发起协商

```sh
# Control Routine
VIRTIOFSD                           QEMU
-------------------------------------------------------------
                              vdc->realize(), i.e., vuf_device_realize()
                                vhost_dev_init()
                                    
                                    .vhost_backend_init()
                            <-      VHOST_USER_GET_FEATURES
send supported features     ->

                            <-      VHOST_USER_GET_PROTOCOL_FEATURES
send protocol features      ->

                            <-      VHOST_USER_SET_PROTOCOL_FEATURES
                                    (send back negotiated protocol features)

                            <-      VHOST_USER_GET_QUEUE_NUM
send @max_queues (i.e., 2)  ->

                            <-      VHOST_USER_GET_MAX_MEM_SLOTS
send VHOST_USER_MAX_RAM_SLOTS ->                            
                            
                            <-      VHOST_USER_SET_SLAVE_REQ_FD
                                    # socketpair(..., sv[2])
                                    # qio_channel->fd = sv[0]
                                    # send sv[1]
dev->slave_fd = sv[1]


                                    .vhost_set_owner()
                            <-      VHOST_USER_SET_OWNER
null op

                                    .vhost_get_features()
                            <-      VHOST_USER_GET_FEATURES
send supported features     ->
                                    
                                    
                                    for each virtqueue
                                    .vhost_set_vring_call(&vq->masked_notifier, ...)
                                    VHOST_USER_SET_VRING_CALL
                                    # send eventfd for call
dev->vq[index].call_fd = vmsg->fds[0]

                              vdc->set_status(), i.e., vuf_set_status()
                                vuf_start() -> vhost_dev_start()
                                    
                                    .vhost_set_mem_table()
                                    VHOST_USER_SET_MEM_TABLE
                                    
                                    for each virtqueue, vhost_virtqueue_start
                                      .vhost_set_vring_num()
                                      VHOST_USER_SET_VRING_NUM
                                      # set size of virtqueue
                                      
                                      .vhost_set_vring_base()
                                      VHOST_USER_SET_VRING_BASE
                                      # set last_avail_idx
                                      
                                      .vhost_set_vring_addr()
                                      VHOST_USER_SET_VRING_ADDR
                                      # set address of three tables
                                      # (descriptor table, available ring, used ring)
                                      
                                      .vhost_set_vring_kick()
                                      VHOST_USER_SET_VRING_KICK
                                      # send eventfd for kick
vu_set_vring_kick_exec
  dev->vq[index].kick_fd = vmsg->fds[0]
  iface->queue_set_started(), i.e., fv_queue_set_started()
    pthread_create(..., fv_queue_thread, ...)
```


这里需要注意的是，对于每一个 virtqueue，qemu 会依次发送 VHOST_USER_SET_VRING_NUM/VHOST_USER_SET_VRING_BASE/VHOST_USER_SET_VRING_ADDR/VHOST_USER_SET_VRING_KICK 等消息，以设置该 virtqueue 的相关配置

virtiofsd 在接收到 VHOST_USER_SET_VRING_KICK 消息后，就会为当前 virtqueue 创建一个 thread 线程（即每个 virtqueue 都会创建这么一个 thread 线程），该线程负责监听、并接收该 virtqueue 上的消息


### Data Routine

#### receive routine

之前在 virtiofs 设备初始化过程中，qemu 会通过 VHOST_USER_SET_VRING_KICK 消息向 virtiofsd 发送一个 fd，之后 qemu 在 virtqueue 中接收到消息的时候，就会通过这个 fd 来通知 virtiofsd

之前介绍过，每个 virtqueue 都会创建一个 thread 线程，负责监听、并接收该 virtqueue 上的消息，这个线程实际上就是通过 poll 监听 VHOST_USER_SET_VRING_KICK 传递过来的 fd

qemu 在 virtqueue 中接收到消息的时候，就会通过这个 fd 来通知 virtiofsd，virtiofsd 依次从 virtqueue 中取出一个个需要处理的 request，之后再调用相应的处理函数进行处理

```sh
# Receive Routine
fv_queue_thread
    # poll on kick fd sent by VHOST_USER_SET_VRING_KICK
    # when POLLIN
      vu_queue_pop // get one request from descriptor table
        fv_queue_worker
          fuse_session_process_buf_int
            fuse_ll_ops[in->opcode].func(req, ...)             
```


另外 virtiofsd 也支持 thread pool，即虽然每个 virtqueue 都只对应一个 thread 线程，但是这个 virtqueue 的负载可以交给 thread pool 处理，用户在编译 virtiofsd 时可以通过 THREAD_POOL_SIZE 设置 thread pool 中 worker thread 的数量，其默认值为 0，即默认不开启 thread pool

```sh
# Receive Routine
fv_queue_thread
    # poll on kick fd sent by VHOST_USER_SET_VRING_KICK
    # when POLLIN
      vu_queue_pop // get one request from descriptor table
      g_thread_pool_push // enqueue into worker thread  

# thread pool        
fv_queue_worker
  fuse_session_process_buf_int
    fuse_ll_ops[in->opcode].func(req, ...)             
```


#### send routine

之前在 virtiofs 设备初始化过程中，qemu 会通过 VHOST_USER_SET_VRING_CALL 消息向 virtiofsd 发送一个 fd，之后 virtiofsd 需要向 virtqueue 写入数据的时候，就会通过这个 fd 来通知 qemu

```sh
fuse_send_reply_iov_nofree
  fuse_send_msg
    virtio_send_msg
      vu_queue_push
        vu_queue_fill // fill corresponding descriptor in descriptor table
        vu_queue_notify
        eventfd_write(vq->call_fd, ...)
```


qemu 中 virtiofs 对应的 virtio_pci 设备会轮询 call fd，一旦接到 virtiofsd 的通知，就会通过中断来通知 guest

```sh
virtio_pci_vector_poll
    vdc->guest_notifier_pending(), i.e., vuf_guest_notifier_pending()
        vhost_virtqueue_pending
            event_notifier_test_and_clear
                msix_set_pending(dev, vector)
```
