title:'io_uring - Feature - Select Buffer'
## io_uring - Feature - Select Buffer

### poll driven retry

之前在介绍 io_uring 的 IO 路径时提到过，在提交处理 sqe 的过程中，首先会尝试 NOWAIT 版本的处理，如果此时返回 -EAGAIN，那么就会进行 retry 重试

一开始 retry 机制就是将这个 sqe 操作 offload 给 io-wq 中的 worker 线程，由 worker 线程进行异步的重试操作，重试过程中会移除 NOWAIT 标记

内核在 v5.7 版本引入一种新的 retry 机制，当首次 NOWAIT 版本的操作返回 -EAGAIN 时，会首先尝试基于 poll 的 retry 重试，即相当于对对应的文件执行 poll 操作，即将当前操作封装为一个实例添加到文件自身维护的一个等待队列中（但是当前进程并不会进入睡眠等待状态，转而继续处理其他的 sqe）；之后当该文件有新数据到来时，就会唤醒等待队列中的实例，从而继续执行相应的操作

目前只有 read/write 以及网络的 connect/accept/receive/send 操作支持这种基于 poll 的 retry 重试，其他操作类型在遇到 -EAGAIN 的时候还是会回退到原始的 io-wq worker 重试


1. register poll instance

```sh
io_queue_sqe
    io_issue_sqe // return -EAGAIN
    io_arm_poll_handler
        __io_arm_poll_handler
            vfs_poll(..., poll_table)
```

vfs_poll() 中会调用 fops->poll() 回调函数，fops->poll() 回调函数一般实现为 poll_wait()，其中会调用传入的 poll_table->_qproc() 回调函数，以将当前 poll 实例添加到该文件自身维护的一个等待队列中

_qproc() 回调函数的原型为

```c
typedef void (*poll_queue_proc)(struct file *, wait_queue_head_t *head, struct poll_table_struct *p);
```

其中传入的 @head 参数即为该文件自身维护的一个等待队列


io_uring 传入的 poll_table._qproc() 回调函数即为 io_async_queue_proc()，其中就是将当前的操作封装为一个 struct io_poll_iocb 结构，并添加到传入的等待队列中，也就是添加到该文件自身维护的一个等待队列中

```sh
io_queue_sqe
    io_issue_sqe // return -EAGAIN
    io_arm_poll_handler
        __io_arm_poll_handler
            vfs_poll(..., poll_table)
                poll_table._qproc(), that is, io_async_queue_proc()
                    add_wait_queue
```

需要注意的是，此时只是将对应的一个 struct io_poll_iocb 实例添加到该文件自身维护的一个等待队列中，当前进程并不会进入睡眠等待状态


2. wake up

之后当文件有新数据到来时，一般就会调用 pollwake()，其中就会依次调用等待队列中的所有节点即 wait_queue_entry 的 func() 回调函数

当初 io_uring 注册的 wait_queue_entry->func() 回调函数为 io_async_wake()，其中会依赖 task_work 机制，由原先首次提交该 sqe 的进程发起重试

```sh
pollwake
    wait_queue_entry->func(), that is, io_async_wake()
        __io_async_wake
            task_work_add
```


### select buffer

以上介绍的基于 poll 的 retry 重试机制，主要应用于 read/write 以及网络的 connect/accept/receive/send 操作

对于这类操作，在 io_uring 出现之前，在传统的编程模型中，通常会首先调用 poll() 系统调用等待文件可读或可写，之后再调用 read()/write() 系统调用完成真正的 IO 操作，这一过程中就需要两个系统调用

而使用 io_uring 编程模型后，以上两步实际上合并为一个系统调用，因而可以提升系统的性能

但是 io_uring 的这一模型还存在一个问题，就是对于 read/receive 这类操作，在传统的编程模型中，只有当 poll() 成功返回，在发起 read/receive 操作的时候才需要分配 user buffer；而在 io_uring 编程模型中，在一开始提交 sqe 的时候就需要事先分配 user buffer，对于高 IOPS 的应用场景来说，这会占用大量的内存

内核在 v5.7 版本引入 buffer selection 特性来解决这一问题，目前该提醒只支持 read/receive 操作


1. register user buffers

应用程序需要首先提交 IORING_OP_PROVIDE_BUFFERS 类型的 sqe 以向内核提前注册一组 user buffer

```c
struct io_uring_sqe {
	__u8	opcode;		/* type of operation for this sqe */
	__s32	fd;		/* file descriptor to do IO on */
	__u64	off;	/* offset into file */
	__u64	addr;	/* pointer to buffer or iovecs */
	__u32	len;		/* buffer size or number of iovecs */
	__u16	buf_group; /* for grouped buffer selection */
	...
}
```

@opcode 必须为 IORING_OP_PROVIDE_BUFFERS

我们将这一组 user buffer 称为是一个 buffer pool，实际上就是一块连续的内存区域，其起始地址为 @addr，这一块内存区域包含了 @fd 个 user buffer，每个 user buffer 的大小为 @len

每个 buffer pool 都有一个编号，即 @buf_group，buffer pool 的划分标准是由应用程序自己定义的，但是在实际应用过程中，通常按照 user buffer 的大小来划分 buffer pool，即将相同大小的一组 user buffer 划分为一个 buffer pool

此外 buffer pool 中的每个 user buffer 也都有一个编号，@off 描述了该 buffer pool 中 user buffer 的起始编号，即该 buffer pool 的第一个 user buffer 的编号为 @off，之后紧邻的下一个 user buffer 的编号为 (@off+1)，以此类推

应用程序需要自己维护所有 user buffer 的编号，即所有 user buffer 都应该具有唯一的编号，因而应用程序需要妥善安排传入的 @off 的值


io_uring 中使用 struct io_buffer 抽象 buffer selection 机制中注册的一个 user buffer

同一个 buffer pool 中的所有 user buffer 对应的 struct io_buffer 组织为一个链表，一个 buffer pool 就对应一个链表

所有 buffer pool 对应的链表都组织在 io_uring 实例的 @io_buffer_idr 字段，对应的 group id 作为 key

```c
struct io_ring_ctx {
	struct idr		io_buffer_idr;
	...
}
```


2. submit sqe

之后引用程序在提交 read/receive 操作类型的 sqe 时

```c
struct io_uring_sqe {
	__u8	flags;		/* IOSQE_ flags */
	__u64	addr;	/* pointer to buffer or iovecs */
	__u32	len;		/* buffer size or number of iovecs */
	__u16	buf_index; /* index into fixed buffers, if used */
	...
}
```

@flags 字段必须设置上 IOSQE_BUFFER_SELECT_BIT 标志，以描述当前操作依赖于 buffer selection 机制分配 user buffer

@buf_index 描述对应的 group id，之后的 buffer selection 机制中就会从 @buf_index group id 描述的 buffer pool 中自动挑选一个空闲可用的 user buffer



此时 @addr/@len 仍是描述用户态的 iovec 数组，只是此时其中的 iovec 不是描述用户态传入的 user buffer，而是作为 output 参数描述 buffer selection 机制分配的 user buffer

由于 buffer selection 机制目前只支持 read/receive 操作，也就是只需要一个 iovec，因而 @len 必须为 1

也就是说此时 @addr 存储的是一个用户态的 struct iovec 结构，需要注意的是，@addr 只是指向 struct iovec 结构体，而无需分配额外的 user buffer

这个 struct iovec 的 @iov_len 字段需要初始化为该操作期待的 user buffer 的大小，@iov_base 字段可以不设置

```c
struct iovec __user *uiov = u64_to_user_ptr(req->rw.addr);
```



3. select free buffer

上述介绍的基于 poll 的 retry 重试机制中，当 poll 返回并由 task_work 机制发起重试时，当检测到 sqe->flags 设置有 IOSQE_BUFFER_SELECT_BIT 标志时，就会通过 buffer selection 机制分配 user buffer

```c
io_import_iovec
    io_rw_buffer_select
        __io_iov_buffer_select
            io_rw_buffer_select
```

其中会从 @buf_index group id 描述的 buffer pool 中自动挑选一个空闲可用的 user buffer

如果 buffer pool 中已经没有更多可用的 user buffer，那么就会返回 -ENOBUFS


4. completion

最终当这个 sqe 重试完成，在填写对应的 cqe 时，对应 cqe 的 @flags 字段实际上设置为

```c
(buffer_id << IORING_CQE_BUFFER_SHIFT) | IORING_CQE_F_BUFFER;
```

IORING_CQE_F_BUFFER 描述这是一个 buffer selection 机制挑选的 user buffer，@buffer_id 描述最终选择的 user buffer 的编号，应用程序需要自己根据这个编号找到对应的 user buffer


需要注意的是，通过 IORING_OP_PROVIDE_BUFFERS 注册的 buffer pool，每个 user buffer 只能用一次，user buffer 用过一次之后就会从 buffer pool 中移除，应用程序需要再次执行 IORING_OP_PROVIDE_BUFFERS 操作，重新注册 user buffer


