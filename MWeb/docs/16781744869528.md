title:'FUSE - Message - Prototype'
## FUSE - Message - Prototype

Filesystem in Userspace (FUSE) 是 server-client 模型，其中 kernel 作为 client，user space daemon 作为 server，两者通过 /dev/fuse 通讯

```
client --[query]--> server --[reply]--> client 
```


### message format

#### basic message format

FUSE 协议中 client/server 之间通过 message 进行通讯，通讯的发起方发送 FUSE request (请求)，之后对方回复 FUSE reply (回复)

FUSE Message (包括 FUSE request/FUSE reply) 的格式均为

```
+-----------+
|   heder   |
+-----------+
|   body    |
+-----------+
```

FUSE request 的头部为

```c
struct fuse_in_header {
	uint32_t	len;
	uint32_t	opcode;
	uint64_t	unique;
	uint64_t	nodeid;
	uint32_t	uid;
	uint32_t	gid;
	uint32_t	pid;
	uint32_t	padding;
};
```


FUSE reply 的头部为

```c
struct fuse_out_header {
	uint32_t	len;
	int32_t		error;
	uint64_t	unique;
};
```

body 部分则是 @opcode 特定的，@opcode 描述当前请求的类型

@unique 是一个用于唯一标识 FUSE request 的 ID，当前所有下发 (但尚未完成) 的 FUSE request 必须具有不同的 @unique

FUSE reply 与 FUSE request 必须具有相同的 @unique 字段，这样才能通过 @unique 字段，定位到当前的 FUSE reply 对应的 FUSE request


#### message format (described by fuse_args)

FUSE 在下发 FUSE request 的时候，需要使用 fuse_args 来描述当前下发的 FUSE request 的相关信息、以及布局等

例如用于下发 FUSE request 的接口有以下两个，都是通过 fuse_args 来描述当前下发的 FUSE request 的相关信息；调用者在调用这两个接口之前，就需要初始化传入的 fuse_args

```c
int fuse_simple_background(struct fuse_mount *fm, struct fuse_args *args, gfp_t gfp_flags)
ssize_t fuse_simple_request(struct fuse_mount *fm, struct fuse_args *args)
```


fuse_args 描述了当前下发的 FUSE request 的相关信息

```c
struct fuse_args {
	uint64_t nodeid;
	uint32_t opcode;
	unsigned short in_numargs;
	unsigned short out_numargs;
	struct fuse_in_arg in_args[3];
	struct fuse_arg out_args[2];
	...
};
```

@opcode 描述了当前下发的 FUSE request 的类型，@nodeid 描述了当前需要对哪个文件 (inode) 执行操作，这个 nodeid 是 LOOKUP 操作过程中 FUSE server 提供的，因而其内部实现是 FUSE server specific 的


接下来的字段描述了 FUSE request 以及对应期待的 FUSE reply 的布局

由于目前 FUSE 协议中，FUSE request 的 body 部分最多包含 3 个不同的元素，FUSE reply 的 body 部分最多包含 2 个不同的元素，而对于某一个具体类型的 FUSE request/FUSE reply，其 body 部分用到的元素数量是不同的，因而 @in_numargs/@out_numargs 就分别描述了当前类型的信息实际用到的元素的数量


FUSE request 的布局为

```
                    args.in_args[0].size args.in_args[1].size args.in_args[2].size
                    <-------------------><------------------><------------------>
+-------------------+-------------------+-------------------+-------------------+
|   fuse_in_header  |       body        |       body        |       body        |
+-------------------+-------------------+-------------------+-------------------+
                    ^                   ^                   ^
                args.in_args[0].value  args.in_args[1].value args.in_args[2].value

```


FUSE reply 的布局为

```
                   args.out_args[0].size  args.out_args[1].size
                    <-------------------><------------------>
+-------------------+-------------------+-------------------+
|  fuse_out_header  |       body        |       body        |
+-------------------+-------------------+-------------------+
                    ^                   ^                   
                args.out_args[0].value  args.out_args[1].value

```


### data routine - message body based

如之前所述，FUSE request message 的三个 body 中，第一个 body 通常存储 opcode specific 的 header，此时 FUSE request message 中还剩余两个 body；

类似地，FUSE reply message 的两个 body 中，第一个 body 通常存储 opcode specific 的 header，此时 FUSE reply message 中还剩余一个 body

此时 FUSE request message 中剩余的两个 body slot 可以用于直接传输其他需要传入的参数，这里一个 body slot 可以用于传输一个 memory segment；类似地，FUSE reply message 中剩余的一个 body slot 可以用于直接传输 daemon 需要传入的其他参数

这种由 FUSE message 中的 body slot 直接描述 client 或 server 需要传输的 memory segment 的形式，就称为 message body based

```
+-------------------+-------------------+-------------------+-------------------+
|   fuse_in_header  |       body        |       body        |       body        |
|                   |  (opcode header)  |      (data)       |      (data)       |
+-------------------+-------------------+-------------------+-------------------+
```

```
+-------------------+-------------------+-------------------+
|  fuse_out_header  |       body        |       body        |
|                   |  (opcode header)  |      (data)       |
+-------------------+-------------------+-------------------+
```


#### request message

client 在下发 FUSE request 之前，可以通过设置 @in_args[] 参数，来描述 client 需要向 daemon 传递的其他参数

```
in_args[1].size =
in_args[1].value = 
fuse_simple_request(...)
```


```
memory topology in kernel:

args.in_args[0].value --->  +-------------------+ 
                            |       body0       | (opcode specific header)
                            +-------------------+
                            args.in_args[0].size

args.in_args[1].value --->  +-------------------+ 
                            |       body1       | (data segment)
                            +-------------------+
                            args.in_args[1].size

args.in_args[2].value --->  +-------------------+ 
                            |       body2       | (data segment)
                            +-------------------+
                            args.in_args[2].size
```


daemon 向 /dev/fuse 执行读操作以获取一个 FUSE request 的时候，就会将上述这些内存依次拷贝到 daemon 传入的缓存中，从而构成一个完整的 FUSE request，其在 daemon 传入的缓存中的布局为

```
memory topology in user's buffer:
+-------------------+-------------------+-------------------+-------------------+
|   fuse_in_header  |       body0       |       body1       |       body2       |
+-------------------+-------------------+-------------------+-------------------+
```


#### reply message

client 在下发 FUSE request 之前，可以通过设置 @out_args[] 参数来描述需要从 daemon 读取的其他参数

```
out_args[1].size =
out_args[1].value = 
fuse_simple_request(...)
```


之后 daemon 在准备好需要返回给 client 的数据后，需要事先在 daemon 的一个缓存中准备好需要返回的 FUSE reply (包括需要返回给 client 的数据)，其布局为

```
memory topology in user's buffer:
+-------------------+-------------------+-------------------+
|  fuse_out_header  |       body0       |       body1       |
+-------------------+-------------------+-------------------+
```

之后 daemon 对 /dev/fuse 执行写操作以写入一个 FUSE reply 的时候，就会将上述 daemon 传入的缓存中的内容依次拷贝到 @out_args[] 描述的内核态缓存中

```
memory topology in kernel:

args.out_args[0].value ---> +-------------------+ 
                            |       body0       | (replied opcode specific header)
                            +-------------------+
                            args.out_args[0].size

args.out_args[1].value ---> +-------------------+ 
                            |       body1       | (replied data)
                            +-------------------+
                            args.out_args[1].size
```


### data routine - iovec based

FUSE message 中可用的 body slot 的数量终归是有限的，即 request message 中连带 header 只有三个 body slot 可用，reply message 中连带 header 只有两个 body slot 可用；如果需要传递的 buffer segment 的数量超过了这些 body slot 的数量，此时就需要用 iovec 来描述需要传递的 buffer segment

此时使用 struct fuse_args_pages 来替代原先的 struct fuse_args 来描述相关的参数

```c
struct fuse_args_pages {
	struct fuse_args args;
	struct page **pages;
	struct fuse_page_desc *descs;
	unsigned int num_pages;
};
```


#### request message

对 FUSE 挂载的文件系统执行 write 类型的系统调用时，

@in_numargs 仍然描述 in args 的数量，只是前面的几个 in args 仍然通过以下方式由 message body 指定

```
in_args[0].size =
in_args[0].value = 
```

其他所有需要传入的数据则是整合为一个 in arg，此时内核会分配足够数量的 page，并将所有这些需要传入的数据 (包括传入的用户态缓存中的数据) 全部拷贝到这些分配的 page 中；此时 @in_numargs 中的最后一个 in arg 就描述这个整合的 in arg

```
ap.args.in_args[ap.args.in_numargs - 1].size = in_size;
ap.args.in_pages = true;
```

此时使用 struct fuse_args_pages 描述相关的参数，@num_pages 描述实际分配的 page 的数量，@in_size 描述这些 page 中所有有效数据（即用户需要写入的数据）的大小；需要注意的是，用户对 FUSE 挂载的文件系统执行 write 类型的系统调用时，可能会传入多个不连续的 buffer segment，这些 buffer segment 中的数据会全部拷贝到分配的 page 中，此时来自多个 buffer segment 的内容可能会拷贝到同一个 page 中

```
memory topology in kernel:

                +---------------+---------------+
                |               |               |
                |               |               |
                |               |               |
                |     page0     |     page1     |
                |               |               |
                |               |               |
                +---------------+---------------+
                        ^               ^ 
                        |               |
ap.pages[] ---> +---------------+---------------+
                | struct page * | struct page * |
                +---------------+---------------+
```

之后 daemon 对 /dev/fuse 执行读操作以获取一个 FUSE request的时候，就会将上述这些内存依次拷贝到 daemon 传入的用户态缓存中，此时传入的用户态缓存中的布局为

```sh
fuse_dev_read
    fuse_dev_do_read
        fuse_copy_args
            fuse_copy_pages
                struct fuse_args_pages *ap = container_of(req->args, ...)
                for each page in @ap.pages[]
                    # copy content of page to user's buffer
```

```
memory topology in user's buffer:
+-------------------+-------------------+-------------------+-------------------+
|   fuse_in_header  |       body0       |       page0       |       page1       |
+-------------------+-------------------+-------------------+-------------------+
```


#### reply message

类似地，对 FUSE 挂载的文件系统执行 read 类型的系统调用时，内核会分配足够数量的 page

@out_numargs 仍然描述 out args 的数量，只是前面的几个 out args 仍然通过以下方式由 message body 指定

```
out_args[0].size =
out_args[0].value = 
```

@out_numargs 中的最后一个 out arg 则描述这个整合的 out arg

```
ap.args.out_args[ap.args.out_numargs - 1].size = out_size;
ap.args.out_pages = true;
```

```
memory topology in kernel:

                +---------------+---------------+
                |               |               |
                |               |               |
                |               |               |
                |     page0     |     page1     |
                |               |               |
                |               |               |
                +---------------+---------------+
                        ^               ^ 
                        |               |
ap.pages[] ---> +---------------+---------------+
                | struct page * | struct page * |
                +---------------+---------------+
```


之后 daemon 在准备好需要返回给 client 的数据后，需要事先在 daemon 的一个缓存中准备好需要返回的 FUSE reply (包括需要返回给 client 的数据)，其布局为

```
memory topology in user's buffer:
+-------------------+-------------------+-------------------+-------------------+
|  fuse_out_header  |       body0       |       page0       |       page1       |
+-------------------+-------------------+-------------------+-------------------+
```

之后 daemon 对 /dev/fuse 执行写操作以写入一个 FUSE reply 的时候，就会将上述用户态缓存中的内容依次拷贝到 @out_args[] 描述的内核缓存中

```sh
fuse_dev_write
    fuse_dev_do_write
        copy_out_args
            fuse_copy_args
                fuse_copy_pages
                    struct fuse_args_pages *ap = container_of(req->args, ...)
                    for each page in @ap.pages[]
                        # copy content in user's buffer to page
```

```
memory topology in kernel:

                +---------------+---------------+
                |               |               |
                |               |               |
                |               |               |
                |     page0     |     page1     |
                |               |               |
                |               |               |
                +---------------+---------------+
                        ^               ^ 
                        |               |
ap.pages[] ---> +---------------+---------------+
                | struct page * | struct page * |
                +---------------+---------------+
```


### copy routine

对 `/dev/fuse` 设备文件进行读写操作时，使用 struct fuse_copy_state 作为 iter 追踪读写操作过程中的内存拷贝

```c
struct fuse_copy_state {
	int write;
	...
};
```

对 /dev/fuse 设备文件进行读操作时，@write 为 true，因而 @write 描述当前是否处理 FUSE_WRITE 请求


#### memory copy for read(2)/write(2)

一般对 /dev/fuse 设备文件进行 read(2)/write(2) 读写操作，都是将数据在内核态缓存与用户传入的用户态缓存之间进行搬运

```c
struct fuse_copy_state {
	struct iov_iter *iter;
	struct page *pg;
	unsigned len;
	unsigned offset;
	...
};
```

此时 @iter 就指向 read(2)/write(2) 传入的 iov_iter，即描述用户态缓存

用户传入的用户态缓存可能是多个 segment，内存拷贝过程中会依次处理这些 segment，此时 @pg/offset/len 就描述当前正在遍历的那个 (user buffer) segment

```
user buffer:
                                    cs->iter
                +---------------+---------------+
                |               |               | 
                |               |***************| +<--- cs->offset
                |               |***************| |
                |     page0     |***************| | cs->len
                |               |***************| +
                |               |               |
                +---------------+---------------+
                                        ^ 
                                        |
                                      cs->pg
```

每次 fuse_copy_fill() 的时候会填充 @pg/offset/len 字段

```
# select one user buffer to copy from/to
fuse_copy_fill(cs)
    # get one user page from cs->iter
    len = iov_iter_get_pages2(cs->iter, &page, ..., &off)
    cs->pg = page
    cs->offset = off
    cs->len = len
```


##### copy message body

fuse_copy_one() 接口用于对 fuse_args 中 in_args[]/out_args[] 描述的内存拷贝到用户态缓存中

```c
struct fuse_args {
	struct fuse_in_arg in_args[3];
	struct fuse_arg out_args[2];
	...
};
```

此时 fuse_copy_state 的 @pg/offset/len 字段描述了用户态缓存，而 fuse_arg 的 @value/size 字段则描述了内核态缓存 (message body)，此时只需要执行简单的 memcpy() 即可实现两者之间的内存拷贝

```sh
fuse_copy_args
    struct fuse_arg *arg = &args[i]
    fuse_copy_one(cs, arg->value, arg->size)
        while size:
            if !cs->len:
                # select one user buffer to copy from/to
                fuse_copy_fill(cs)
            
            # copy to/from user buffer
            fuse_copy_do(cs, &val, &size)
                void *pgaddr = kmap_local_page(cs->pg);
                void *buf = pgaddr + cs->offset;
            
            if cs->write:
                memcpy(buf, *val, ncpy);
            else:
                memcpy(*val, buf, ncpy);
            
            kunmap_local(pgaddr);
            cs->len -= ncpy;
            cs->offset += ncpy;
```

##### copy page

fuse_copy_pages() 接口用于对 page-based 描述的内核缓存到用户态缓存之间的内存拷贝

此时类似地 fuse_copy_state 的 @pg/offset/len 字段描述了用户态缓存，而 ap->pages[i] 则描述了内核态缓存，此时只需要执行简单的 memcpy() 即可实现两者之间的内存拷贝

```sh
fuse_copy_args
    struct fuse_arg *arg = &args[i]
    fuse_copy_pages(cs, arg->size, zeroing)
        struct fuse_args_pages *ap = #... derived from fuse_req
        # for each page in the fuse request:
            offset = ap->descs[i].offset
            page = &ap->pages[i]
            
            fuse_copy_page(cs, page, offset, ...)
                # mapping kernel page
                *mapaddr = kmap_local_page(page)
                *buf = mapaddr + offset
                
                offset += fuse_copy_do(cs, &buf, &count)
                    # mapping user page
                    void *pgaddr = kmap_local_page(cs->pg);
                    void *buf = pgaddr + cs->offset;
                    # do memcpy() ...
                    kunmap_local(pgaddr);
                
                kunmap_local(mapaddr)
```


#### memory copy for splice(2)

/dev/fuse 设备文件还支持 splice(2) 模式，从而支持例如 zero-copy 特性等

##### zero-copy for FUSE_READ reply

利用 splice(2) 实现 FUSE_READ reply 的 zero-copy 的流程如下

```
fuse_dev = open("/dev/fuse", ...)
pipe2(pip[2])                 # temporary pipe
src_file = open("<src_file>", ...) # source underlying file (e.g. ext4)

# 1. setup fuse reply in temporary pipe
struct fuse_out_header out;
iov.iov_base = &out;
iov.iov_len = sizeof(struct fuse_out_header);
vmsplice(pip[1], &iov, 1, 0);

# 2. move data from underlying file's page cache to temporary pipe
splice(src_file, ..., pip[1], ...)

/* retrospectively modify the header: */
out.len = len + sizeof(struct fuse_out_header);

# 2. then move data from pipe to corresponding fuse file
splice(pip[0], ..., fuse_dev, ..., flags) 
```

> 1. move data from underlying file's page cache to temporary pipe

```
splice(src_file, ..., pip[1], ...)
```

首先执行一次 splice(2) 将需要读取的对应底层文件 (例如 ext4 文件) 的 page cache 暂时缓存在一个临时的 pipe 中

这里需要注意的是，pipe 内部通过 @bufs[] 数组来管理该 pipe 的所有数据

```c
struct pipe_inode_info {
	struct pipe_buffer *bufs;
	...
};
```

上面介绍的 splice 过程实际上是使得 pipe 的 @bufs[] 数组引用底层文件的 page cache，这一过程中并不涉及任何的内存拷贝

```
# 1. move data from underlying file's page cache to temporary pipe
splice(src_file, ..., pip[1], ...)
    vfs_splice_read
        src file's .splice_read(), e.g. ext4_file_splice_read()
            filemap_splice_read
                # find src file's page cache
                filemap_get_pages
                
                # fill pipe (make pipe refer to src file's page cache)
                splice_folio_into_pipe
                    struct pipe_buffer *buf = pipe_head_buf(pipe)
                    page = folio_page(folio, ...)
                    buf->page = page
                    folio_get(folio)
```

如下所示，pipe 中缓存的数据，实际上只是对底层文件的 page cache 的一个引用，其自身并不占用任何额外的内存

```
underlying ext4                                     fuse
page cache:                 temporary pipe:         page cache:

+---------------+
|               |
|               |
|               | <----------  bufs[]
|      page     | folio_get()
|               |
|               |
+---------------+
```


> 2. move data from pipe to corresponding fuse file

```
splice(pip[0], ..., fuse_dev, ..., flags) 
```

接下来执行一次 splice 将 pipe 中缓存的数据移到对应的 fuse 文件的 address space 中

```
# 2. then move data from pipe to corresponding fuse file
splice(pip[0], ..., fuse_dev, ..., flags) 
    do_splice_from
        fuse_fd's .splice_write(), i.e. fuse_dev_splice_write()
            fuse_dev_do_write
                # read reply header from pipe, and then find corresponding fuse request bt req->unique id
                
                copy_out_args
                    fuse_copy_args
                        fuse_copy_pages
                            fuse_copy_page
                                if flags & SPLICE_F_MOVE, i.e cs->move_pages is true,
                                   and page to be copied to (fuse file's page cache) is a full page (offset == 0 && count == PAGE_SIZE):
                                    fuse_try_move_page
                                        # attempt to take ownership of a pipe_buffer. i.e. remove this page from its original (underlying file's, i.e. ext4) address space if any
                                        pipe_buf_try_steal(pipe, buf)
                                            buf->ops->try_steal(), i.e. page_cache_pipe_buf_try_steal()
                                                remove_mapping(mapping, folio)
                                        
                                        oldfolio = ... # from req->ap
                                        newfolio = ... # from pipe
                                        
                                        # replace a pagecache folio (previous allocated placeholder) with a new one (fetched from underlying file's page cache)
                                        replace_page_cache_folio(oldfolio, newfolio)
                                            xas_store(&xas, new) # store new folio in address space
                                            folio_put(old) # free old folio                                                
```

pipe 中缓存的 page 是从底层文件 (例如 ext4 文件) 获取的，其实际上是底层文件 (例如 ext4 文件) 的 address space 中的一个 page cache，此时需要将这个 page 转移到对应 fuse 文件的 address space 中

这一过程中当 splice(2) 传入的 flags 设置有 SPLICE_F_MOVE 的时候，就会开启 zero copy 特性，这一过程中除了将这个 page 从 pipe 管理的 bufs[] 数组移除，还需要将其从原来的 (例如原来 ext4 文件) address space 中移除，然后再将这个 page 添加到对应 fuse 文件的 address space 中，从而实现 zero-copy

```
underlying ext4                                     fuse
page cache:                 temporary pipe:         page cache:

                                                    +---------------+
                                                    |               |
                                                    |               |
                               bufs[]               |               |
                                                    |      page     |
                                                    |               |
                                                    |               |
                                                    +---------------+
```

如果上述将 page 从原来的 (例如原来 ext4 文件) address space 中移除的操作失败、或者这个 page 不是一个完整的 PAGE_SIZE 的 full page (例如只是一个 page 当中的一部分 segment)、或者 splice(2) 没有设置 SPLICE_F_MOVE 标志，那么都会回退到 non-zero-copy

```
underlying ext4                     fuse
page cache:                         page cache:

+---------------+                   +---------------+
|               |                   |               |
|               |       copy        |               |
|               |   ------------>   |               |
|      page     |                   |      page     |
|               |                   |               |
|               |                   |               |
+---------------+                   +---------------+
        ^                                  ^
    pipe->bufs[]                        req->ap[]
```


##### zero-copy for FUSE_WRITE

利用 splice(2) 实现 FUSE_WITE 的 zero-copy 的流程如下

```
fuse_dev = open("/dev/fuse", ...)
pipe2(pip[2])                 # temporary pipe

# 1. move data from fuse device to temporary pipe
splice(fuse_dev, ..., pip[1], ..., flags)

# 2. read the header from the pipe
read(pip[0], ...)
# then parse the header and determine the message type

# if it's a WRITE message, move data from temporary pipe to underlying file's page cache:
dest_file = open("<src_file>", ...) # destination underlying file (e.g. ext4)
splice(pip[0], ..., dest_file, ...)
```

> The semantics of using splice() to read messages are:
>
> 1)  with a single splice()  move the whole message from the fuse
>     device to a temporary pipe
> 2)  read the header from the pipe and determine the message type
> 3a) if message is a WRITE then splice data from pipe to destination
> 3b) else read the rest of the message


> move data from fuse device to temporary pipe

```
splice(fuse_dev, ..., pip[1], ..., flags)
```

首先执行一次 splice(2) 将 fuse 设备文件的输出链接到 pipe，也就是将一个 fuse 请求 (包括 header 以及 (如果是 FUSE_WRITE 请求) 后面的 page 数据) 暂时缓存在一个临时的 pipe 中

```
 # move data from fuse device to temporary pipe
splice(fuse_dev, ..., pip[1], ..., flags)
    splice_file_to_pipe
        vfs_splice_read
            fuse device file's f_op->splice_read(in, ..., pipe, ...), i.e. fuse_dev_splice_read()
                fuse_dev_do_read
                    # get one request to be handled
                    req = ...
                    
                    fuse_copy_args
                        fuse_copy_pages
                            fuse_copy_page
                                # if !req->args->user_pages:
                                fuse_ref_page(cs, page, ...)
                                    get_page(page)
                                    pipebuf->page = page
                                
```

如果 fuse 是 writeback 模式，那么是在回写流程中发起的 FUSE_WRITE 操作，此时 fuse 请求中 req->ap[] 指向的都是 fuse 文件的 page cache；那么此时 splice 操作中，pipe 的 bufs[] 数组实际上就只是指向 fuse 文件的 page cache

```
fuse                                                underlying ext4
page cache:                 temporary pipe:         page cache:

+---------------+
|               |
|               |
|               | <----------  bufs[]
|      page     | get_page()
|               |
|               |
+---------------+
        ^
    req->ap[]
```


> move data from temporary pipe to underlying file's page cache:

```
dest_file = open("<src_file>", ...) # destination underlying file (e.g. ext4)
splice(pip[0], ..., dest_file, ...)
```

接下来执行一次 splice 将 pipe 中缓存的数据 (即 fuse 文件的 page cache) 移到对应的 underlying 文件的 address space 中

```
# move data from temporary pipe to underlying file's page cache:
splice(pip[0], ..., dest_file, ...)
    do_splice_from
        dest_fd's .splice_write(), e.g. iter_file_splice_write() for ext4 file
            vfs_iter_write()
```

但是需要注意的是，ext4 的 .splice_write() 不支持 zero-copy 特性

```
fuse                                underlying ext4
page cache:                         page cache:

+---------------+                   +---------------+
|               |                   |               |
|               |       copy        |               |
|               |   ------------>   |               |
|      page     |  vfs_iter_write() |      page     |
|               |                   |               |
|               |                   |               |
+---------------+                   +---------------+
        ^                                  ^
    pipe->bufs[]                        
    req->ap[]
```